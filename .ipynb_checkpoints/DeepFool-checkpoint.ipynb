{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a30c9b0",
   "metadata": {},
   "source": [
    "### Deep Fool Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113174d9",
   "metadata": {},
   "source": [
    "We have achieved some results with the CNN and MLP models, but we would like to explore how these models perform when faced with adversarial examples. To do so, we aply DeepFool.\n",
    "\n",
    "**DeepFool** is a widely used method for generating adversarial examples, designed to evaluate the robustness of machine learning models, particularly in classification tasks. \n",
    "\n",
    "By iteratively finding the minimal perturbation needed to alter a model's prediction, DeepFool provides insights into a **model's vulnerability to adversarial attacks**. This method is crucial for developing more **robust and reliable models**, as it helps identify potential weaknesses and informs strategies for improving their defense mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b642f6",
   "metadata": {},
   "source": [
    "![Descrição da imagem](./images/deepfool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfcf90",
   "metadata": {},
   "source": [
    "\n",
    "1. **Initialization:** Starting with the original image \\( $x_0$ \\), and setting the iteration counter \\($ i $\\) to 0.\n",
    "\n",
    "2. **Perturbation Calculation:** For each iteration:\n",
    "   - The algorithm calculates the perturbation required to change the model's prediction, iteratively adjusting the image.\n",
    "   - It computes the gradients and the necessary perturbation \\( $r_i $\\) for the class boundary.\n",
    "\n",
    "3. **Repeat Until Misclassification:** The algorithm repeats this process until the image is misclassified by the model.\n",
    "\n",
    "4. **Return Perturbation:** The final perturbation is the sum of all the adjustments \\( $r_i$ \\) made during the iterations.\n",
    "\n",
    "The result is the minimal perturbation $ r $ that causes a misclassification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d1ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of relevant libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, regularizers, optimizers\n",
    "from tensorflow.python.client import device_lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import librosa\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b56bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(model, x, target_class):\n",
    "    \"\"\"\n",
    "    Calcula o gradiente da saída da classe-alvo com respeito à entrada.\n",
    "\n",
    "    Args:\n",
    "        model: O modelo neural treinado.\n",
    "        x: Entrada para o modelo (tensor).\n",
    "        target_class: Índice da classe-alvo.\n",
    "\n",
    "    Returns:\n",
    "        Gradiente calculado.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        logits = model(x)\n",
    "        target_logits = logits[:, target_class]\n",
    "    return tape.gradient(target_logits, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9076828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def deepfool(model, x0, eta=1e-2, max_iter=50, num_classes=10):\n",
    "    \"\"\"\n",
    "    Implementa o algoritmo DeepFool para calcular a menor perturbação.\n",
    "\n",
    "    Args:\n",
    "        model: O modelo neural treinado.\n",
    "        x0: Entrada inicial (tensor).\n",
    "        eta: Parâmetro de overshoot.\n",
    "        max_iter: Número máximo de iterações.\n",
    "        num_classes: Número de classes no modelo.\n",
    "\n",
    "    Returns:\n",
    "        r_sum: Perturbação acumulada.\n",
    "        loop_i: Número de iterações realizadas.\n",
    "        label_xi: Nova previsão após a perturbação.\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x0, dtype=tf.float32)\n",
    "    r_sum = tf.zeros_like(x)\n",
    "    label_xi = tf.argmax(model(x), axis=1).numpy()[0]\n",
    "\n",
    "    for loop_i in range(max_iter):\n",
    "        gradients = []\n",
    "        logits = model(x)\n",
    "        current_label = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "        if current_label != label_xi:\n",
    "            break\n",
    "\n",
    "        for k in range(num_classes):\n",
    "            grad = get_gradient(model, x, k)\n",
    "            gradients.append(grad)\n",
    "\n",
    "        gradients = tf.stack(gradients)\n",
    "        logits = tf.squeeze(logits)\n",
    "\n",
    "        smallest_perturbation = float('inf')\n",
    "        for k in range(num_classes):\n",
    "            if k == label_xi:\n",
    "                continue\n",
    "            w_k = gradients[k] - gradients[label_xi]\n",
    "            f_k = logits[k] - logits[label_xi]\n",
    "            perturbation = tf.abs(f_k) / tf.norm(w_k, ord=2)\n",
    "            if perturbation < smallest_perturbation:\n",
    "                smallest_perturbation = perturbation\n",
    "                r_i = (perturbation + eta) * w_k / tf.norm(w_k, ord=2)\n",
    "\n",
    "        r_sum += r_i\n",
    "        x = x + r_i\n",
    "\n",
    "    return r_sum, loop_i, current_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d72d9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_robustness(r, x):\n",
    "    \"\"\"\n",
    "    Calcula a robustez de um exemplo.\n",
    "\n",
    "    Args:\n",
    "        r: Perturbação adversarial aplicada.\n",
    "        x: Entrada original.\n",
    "\n",
    "    Returns:\n",
    "        Valor de robustez (ρ).\n",
    "    \"\"\"\n",
    "    norm_r = tf.norm(r)\n",
    "    norm_x = tf.norm(x)\n",
    "    return norm_r / norm_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8803013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_robustness(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Avalia a robustez média do modelo em relação ao conjunto de testes.\n",
    "\n",
    "    Args:\n",
    "        model: O modelo neural treinado.\n",
    "        X_test: Conjunto de dados de teste.\n",
    "        y_test: Labels do conjunto de teste.\n",
    "\n",
    "    Returns:\n",
    "        Média e desvio padrão da robustez.\n",
    "    \"\"\"\n",
    "    rho_values = []\n",
    "    for i in range(len(X_test)):\n",
    "        x = tf.expand_dims(X_test[i], axis=0)\n",
    "        r, _, _ = deepfool(model, x)\n",
    "        rho = example_robustness(r, x)\n",
    "        rho_values.append(rho.numpy())\n",
    "    \n",
    "    mean_rho = np.mean(rho_values)\n",
    "    std_rho = np.std(rho_values)\n",
    "    return mean_rho, std_rho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a00b8f-86f5-4ab3-8ab6-a6bbf6b70681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar robustez do modelo\n",
    "mean_rho, std_rho = model_robustness(model, X_test, y_test)\n",
    "print(f\"Medium robustness: {mean_rho:.4f}\")\n",
    "print(f\"Standard deviation of robustness: {std_rho:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61ea02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.1474 - loss: 2.2515 - val_accuracy: 0.3350 - val_loss: 2.1373\n",
      "Epoch 2/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.3759 - loss: 2.0446 - val_accuracy: 0.4127 - val_loss: 1.9569\n",
      "Epoch 3/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4481 - loss: 1.8074 - val_accuracy: 0.3822 - val_loss: 1.7841\n",
      "Epoch 4/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4825 - loss: 1.6219 - val_accuracy: 0.4854 - val_loss: 1.6441\n",
      "Epoch 5/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5295 - loss: 1.4747 - val_accuracy: 0.5108 - val_loss: 1.5327\n",
      "Epoch 6/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5575 - loss: 1.3904 - val_accuracy: 0.5299 - val_loss: 1.4604\n",
      "Epoch 7/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5821 - loss: 1.3145 - val_accuracy: 0.5554 - val_loss: 1.3781\n",
      "Epoch 8/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5920 - loss: 1.2569 - val_accuracy: 0.5656 - val_loss: 1.3274\n",
      "Epoch 9/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.6026 - loss: 1.2196 - val_accuracy: 0.5898 - val_loss: 1.2733\n",
      "Epoch 10/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6130 - loss: 1.1753 - val_accuracy: 0.5860 - val_loss: 1.2287\n",
      "Epoch 11/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.6177 - loss: 1.1475 - val_accuracy: 0.5924 - val_loss: 1.1794\n",
      "Epoch 12/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6264 - loss: 1.1216 - val_accuracy: 0.6013 - val_loss: 1.1530\n",
      "Epoch 13/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6503 - loss: 1.0743 - val_accuracy: 0.6038 - val_loss: 1.1522\n",
      "Epoch 14/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6479 - loss: 1.0594 - val_accuracy: 0.6089 - val_loss: 1.1144\n",
      "Epoch 15/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6583 - loss: 1.0391 - val_accuracy: 0.6293 - val_loss: 1.0816\n",
      "Epoch 16/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6721 - loss: 1.0108 - val_accuracy: 0.6981 - val_loss: 1.0336\n",
      "Epoch 17/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.6641 - loss: 1.0089 - val_accuracy: 0.7134 - val_loss: 1.0076\n",
      "Epoch 18/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6864 - loss: 0.9751 - val_accuracy: 0.7032 - val_loss: 1.0195\n",
      "Epoch 19/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.6791 - loss: 0.9631 - val_accuracy: 0.7172 - val_loss: 0.9764\n",
      "Epoch 20/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6825 - loss: 0.9650 - val_accuracy: 0.7248 - val_loss: 0.9623\n"
     ]
    }
   ],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units, dropout_rate, activations, regularization_type=None, regularization_value=0.01):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = []\n",
    "        self.regularization_type = regularization_type\n",
    "        self.regularization_value = regularization_value\n",
    "\n",
    "        for units, activation in zip(hidden_units, activations):\n",
    "            self.hidden_layers.append(\n",
    "                tf.keras.layers.Dense(units, activation=activation)\n",
    "            )\n",
    "            self.hidden_layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, activation='softmax')  \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def compute_regularization_loss(self):\n",
    "        regularization_loss = 0.0\n",
    "        if self.regularization_type:\n",
    "            for layer in self.hidden_layers:\n",
    "                if isinstance(layer, tf.keras.layers.Dense):\n",
    "                    weights = layer.kernel\n",
    "                    if self.regularization_type == 'l1':\n",
    "                        regularization_loss += tf.reduce_sum(tf.abs(weights)) * self.regularization_value\n",
    "                    elif self.regularization_type == 'l2':\n",
    "                        regularization_loss += tf.reduce_sum(tf.square(weights)) * self.regularization_value\n",
    "        return regularization_loss\n",
    "\n",
    "def load_fold_data(fold_index, files):\n",
    "    # Adjust fold_index to be zero-based\n",
    "    data = pd.read_csv(files[fold_index-1]).to_numpy()\n",
    "\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Warning: Missing values detected in file {files[fold_index - 1]}.\")\n",
    "        data = data[~np.isnan(data).any(axis=1)]  # Remove rows with NaN values\n",
    "    X = data[:, :-1]  # Features\n",
    "    y = data[:, -1].astype(int)  # Labels\n",
    "    if (y < 0).any() or (y >= 10).any():\n",
    "        raise ValueError(f\"Invalid label values detected in file {files[fold_index - 1]}. Labels: {np.unique(y)}\")\n",
    "    return X, y\n",
    "\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "\n",
    "# Define the test fold\n",
    "fold_test = 1\n",
    "X_test, y_test = load_fold_data(fold_test, files)\n",
    "\n",
    "# Define the training folds\n",
    "X_train, y_train = [], []\n",
    "for i in range(1, 11):  # Total of 10 folds\n",
    "    if i != fold_test:\n",
    "        X_temp, y_temp = load_fold_data(i, files)\n",
    "        X_train.append(X_temp)\n",
    "        y_train.append(y_temp)\n",
    "\n",
    "# Concatenate the training data\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "# Hyperparameters\n",
    "best_config = {\n",
    "    'hidden_units': [256, 128, 64],\n",
    "    'activations': ['relu', 'relu', 'relu'],\n",
    "    'dropout_rate': 0.3,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.0001,\n",
    "    'regularization_type': None,\n",
    "    'regularization_value': 0.01\n",
    "}\n",
    "\n",
    "# Initialize and train the model\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=10,  # Classes from 0 to 9\n",
    "    hidden_units=best_config['hidden_units'],\n",
    "    dropout_rate=best_config['dropout_rate'],\n",
    "    activations=best_config['activations']\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_config['learning_rate']),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Example validation split\n",
    "X_val, y_val = X_train[:len(X_train)//10], y_train[:len(y_train)//10]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=best_config['batch_size'],\n",
    "    epochs=best_config['epochs']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "067d6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfool_mlp(model, x0, y0, max_iter=50, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Implements the DeepFool adversarial attack for a given model and input.\n",
    "    \"\"\"\n",
    "    x_adv = tf.identity(x0)  # Copy input tensor for adversarial manipulation\n",
    "    logits = model(x_adv)  # Get model predictions\n",
    "    pred_label = tf.argmax(logits, axis=-1).numpy()[0]\n",
    "    \n",
    "    if pred_label != y0:\n",
    "        return x0.numpy()  # Return original input if already misclassified\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            logits = model(x_adv)\n",
    "\n",
    "        gradients = tape.gradient(logits, x_adv).numpy()\n",
    "        logits = logits.numpy()[0]\n",
    "        current_label = np.argmax(logits)\n",
    "        if current_label != y0:\n",
    "            break  # Misclassification achieved\n",
    "\n",
    "        w = gradients - gradients[y0]\n",
    "        f = logits - logits[y0]\n",
    "\n",
    "        perturbations = []\n",
    "        for k in range(len(logits)):\n",
    "            if k != y0:\n",
    "                norm_w = np.linalg.norm(w[k]) + epsilon\n",
    "                perturbations.append((abs(f[k]) / norm_w, k))\n",
    "\n",
    "        perturbations.sort()\n",
    "        r_min, k_min = perturbations[0]\n",
    "        x_adv += (1 + epsilon) * tf.convert_to_tensor(r_min * w[k_min], dtype=tf.float32)\n",
    "\n",
    "    return x_adv.numpy()\n",
    "\n",
    "def cross_validation_mlp_deepfool(datasets, model_builder, params):\n",
    "    accuracy_values = []\n",
    "    loss_values = []\n",
    "    robustness_values = []\n",
    "\n",
    "    for i, test_set in enumerate(datasets):\n",
    "        print(f\"=== Fold {i+1} ===\")\n",
    "\n",
    "        # Prepare the train, validation, and test splits\n",
    "        validation_set = datasets[(i + 1) % len(datasets)]\n",
    "        train_set = pd.concat([datasets[j] for j in range(len(datasets)) if j != i and j != (i + 1) % len(datasets)])\n",
    "\n",
    "        X_train, y_train = train_set.iloc[:, :-1].values, train_set.iloc[:, -1].values\n",
    "        X_val, y_val = validation_set.iloc[:, :-1].values, validation_set.iloc[:, -1].values\n",
    "        X_test, y_test = test_set.iloc[:, :-1].values, test_set.iloc[:, -1].values\n",
    "\n",
    "        # Build and train the model\n",
    "        model = model_builder(\n",
    "            input_dim=X_train.shape[1],\n",
    "            output_dim=10,  # Classes from 0 to 9\n",
    "            hidden_units=best_config['hidden_units'],\n",
    "            dropout_rate=best_config['dropout_rate'],\n",
    "            activations=best_config['activations']\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=params['batch_size'],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Evaluate the model on test data\n",
    "        fold_loss, fold_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        accuracy_values.append(fold_accuracy)\n",
    "        loss_values.append(fold_loss)\n",
    "\n",
    "        # Evaluate robustness using DeepFool\n",
    "        adversarial_success = 0\n",
    "        for idx in range(len(X_test)):\n",
    "            x0 = np.expand_dims(X_test[idx], axis=0)\n",
    "            y0 = y_test[idx]\n",
    "            x_adv = deepfool_mlp(model, x0, y0)\n",
    "            adv_pred = tf.argmax(model(x_adv), axis=-1).numpy()[0]\n",
    "\n",
    "            if adv_pred != y0:\n",
    "                adversarial_success += 1\n",
    "\n",
    "        robustness = 1 - (adversarial_success / len(X_test))\n",
    "        robustness_values.append(robustness)\n",
    "        print(f\"Robustness for fold {i+1}: {robustness:.4f}\")\n",
    "\n",
    "    return accuracy_values, loss_values, robustness_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "253acf75-5348-40b7-8e8e-a6fd1ec4d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, regularizers\n",
    "\n",
    "def build_mlp_model(input_shape, hidden_units, learning_rate, dropout_rate):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    # Add hidden layers\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(units, activation='relu', \n",
    "                               kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # Assuming 10 classes\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=best_config['learning_rate']),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c901e4d5-e1ef-4099-af8b-42ca1a7f37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "params = {\n",
    "    'hidden_units': [256, 128, 64],\n",
    "    'activations': ['relu', 'relu', 'relu'],\n",
    "    'dropout_rate': 0.3,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.0001,\n",
    "    'regularization_type': None,\n",
    "    'regularization_value': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "412a6313-52b8-4aa1-a076-044b13305b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all folds into a list\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "folds = [pd.read_csv(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a6fb1ec-19bf-4d18-9293-726c47948431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fold 1 ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "build_mlp_model() missing 2 required positional arguments: 'l2_lambda' and 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy_values, loss_values, robustness_values \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_mlp_deepfool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_builder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_mlp_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 55\u001b[0m, in \u001b[0;36mcross_validation_mlp_deepfool\u001b[1;34m(datasets, model_builder, params)\u001b[0m\n\u001b[0;32m     52\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m test_set\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, test_set\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Build and train the model\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_units\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     63\u001b[0m     X_train, y_train,\n\u001b[0;32m     64\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test data\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: build_mlp_model() missing 2 required positional arguments: 'l2_lambda' and 'optimizer'"
     ]
    }
   ],
   "source": [
    "accuracy_values, loss_values, robustness_values = cross_validation_mlp_deepfool(\n",
    "    datasets=folds,\n",
    "    model_builder=build_mlp_model,\n",
    "    params=params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba7109-00f5-40e1-b8ff-5015bf13c296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
