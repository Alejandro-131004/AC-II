{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a30c9b0",
   "metadata": {},
   "source": [
    "### DeepFool Algorithm <a name=\"deepfool\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113174d9",
   "metadata": {},
   "source": [
    "We have achieved some results with the CNN and MLP models, but we would like to explore how these models perform when faced with adversarial examples. To do so, we aply DeepFool.\n",
    "\n",
    "**DeepFool** is a widely used method for generating adversarial examples, designed to evaluate the robustness of machine learning models, particularly in classification tasks. \n",
    "\n",
    "By iteratively finding the minimal perturbation needed to alter a model's prediction, DeepFool provides insights into a **model's vulnerability to adversarial attacks**. This method is crucial for developing more **robust and reliable models**, as it helps identify potential weaknesses and informs strategies for improving their defense mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b642f6",
   "metadata": {},
   "source": [
    "![Descrição da imagem](./images/deepfool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfcf90",
   "metadata": {},
   "source": [
    "\n",
    "1. **Initialization:** Starting with the original image \\( $x_0$ \\), and setting the iteration counter \\($ i $\\) to 0.\n",
    "\n",
    "2. **Perturbation Calculation:** For each iteration:\n",
    "   - The algorithm calculates the perturbation required to change the model's prediction, iteratively adjusting the image.\n",
    "   - It computes the gradients and the necessary perturbation \\( $r_i $\\) for the class boundary.\n",
    "\n",
    "3. **Repeat Until Misclassification:** The algorithm repeats this process until the image is misclassified by the model.\n",
    "\n",
    "4. **Return Perturbation:** The final perturbation is the sum of all the adjustments \\( $r_i$ \\) made during the iterations.\n",
    "\n",
    "The result is the minimal perturbation $ r $ that causes a misclassification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93c37f-f1f0-46b6-a131-419f6d3a6744",
   "metadata": {},
   "source": [
    "#### Imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d1ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, regularizers, optimizers\n",
    "from tensorflow.python.client import device_lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import librosa\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbcaa8-7882-4216-808c-2b2b82692393",
   "metadata": {},
   "source": [
    "#### Class MLP\n",
    "[[go back to the top]](#deepfool)\n",
    "\n",
    "The MLP model, similar to the one implemented in the [[MLP]](./MLP.ipynb) file, is built with the best hyperparameter configuration obtained previously and trained with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ea02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1875 - loss: 2.2475 - val_accuracy: 0.2917 - val_loss: 2.1037\n",
      "Epoch 2/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3056 - loss: 2.0502 - val_accuracy: 0.3567 - val_loss: 1.9065\n",
      "Epoch 3/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4009 - loss: 1.8247 - val_accuracy: 0.4420 - val_loss: 1.7311\n",
      "Epoch 4/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4830 - loss: 1.6741 - val_accuracy: 0.4420 - val_loss: 1.6252\n",
      "Epoch 5/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5298 - loss: 1.5154 - val_accuracy: 0.4943 - val_loss: 1.4976\n",
      "Epoch 6/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5504 - loss: 1.4208 - val_accuracy: 0.5121 - val_loss: 1.4090\n",
      "Epoch 7/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5720 - loss: 1.3396 - val_accuracy: 0.5580 - val_loss: 1.3154\n",
      "Epoch 8/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5914 - loss: 1.2700 - val_accuracy: 0.6038 - val_loss: 1.2624\n",
      "Epoch 9/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6153 - loss: 1.1980 - val_accuracy: 0.5694 - val_loss: 1.2334\n",
      "Epoch 10/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6217 - loss: 1.1582 - val_accuracy: 0.6268 - val_loss: 1.1823\n",
      "Epoch 11/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6329 - loss: 1.1042 - val_accuracy: 0.6522 - val_loss: 1.1494\n",
      "Epoch 12/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6415 - loss: 1.0856 - val_accuracy: 0.6420 - val_loss: 1.1320\n",
      "Epoch 13/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6376 - loss: 1.0864 - val_accuracy: 0.6815 - val_loss: 1.0905\n",
      "Epoch 14/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6535 - loss: 1.0493 - val_accuracy: 0.7019 - val_loss: 1.0757\n",
      "Epoch 15/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6574 - loss: 1.0345 - val_accuracy: 0.6879 - val_loss: 1.0265\n",
      "Epoch 16/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6635 - loss: 1.0138 - val_accuracy: 0.7057 - val_loss: 1.0247\n",
      "Epoch 17/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6632 - loss: 1.0095 - val_accuracy: 0.7261 - val_loss: 0.9899\n",
      "Epoch 18/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6784 - loss: 0.9641 - val_accuracy: 0.6943 - val_loss: 0.9874\n",
      "Epoch 19/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6800 - loss: 0.9541 - val_accuracy: 0.7261 - val_loss: 0.9442\n",
      "Epoch 20/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6953 - loss: 0.9209 - val_accuracy: 0.7274 - val_loss: 0.9322\n"
     ]
    }
   ],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units, dropout_rate, activations, regularization_type=None, regularization_value=0.01):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = []\n",
    "        self.regularization_type = regularization_type\n",
    "        self.regularization_value = regularization_value\n",
    "\n",
    "        for units, activation in zip(hidden_units, activations):\n",
    "            self.hidden_layers.append(\n",
    "                tf.keras.layers.Dense(units, activation=activation)\n",
    "            )\n",
    "            self.hidden_layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, activation='softmax')  \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def compute_regularization_loss(self):\n",
    "        regularization_loss = 0.0\n",
    "        if self.regularization_type:\n",
    "            for layer in self.hidden_layers:\n",
    "                if isinstance(layer, tf.keras.layers.Dense):\n",
    "                    weights = layer.kernel\n",
    "                    if self.regularization_type == 'l1':\n",
    "                        regularization_loss += tf.reduce_sum(tf.abs(weights)) * self.regularization_value\n",
    "                    elif self.regularization_type == 'l2':\n",
    "                        regularization_loss += tf.reduce_sum(tf.square(weights)) * self.regularization_value\n",
    "        return regularization_loss\n",
    "\n",
    "def load_fold_data(fold_index, files):\n",
    "    # Adjust fold_index to be zero-based\n",
    "    data = pd.read_csv(files[fold_index-1]).to_numpy()\n",
    "\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Warning: Missing values detected in file {files[fold_index - 1]}.\")\n",
    "        data = data[~np.isnan(data).any(axis=1)]  # Remove rows with NaN values\n",
    "    X = data[:, :-1]  # Features\n",
    "    y = data[:, -1].astype(int)  # Labels\n",
    "    if (y < 0).any() or (y >= 10).any():\n",
    "        raise ValueError(f\"Invalid label values detected in file {files[fold_index - 1]}. Labels: {np.unique(y)}\")\n",
    "    return X, y\n",
    "\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "\n",
    "# Define the test fold\n",
    "fold_test = 1\n",
    "X_test, y_test = load_fold_data(fold_test, files)\n",
    "\n",
    "# Define the training folds\n",
    "X_train, y_train = [], []\n",
    "for i in range(1, 11):  # Total of 10 folds\n",
    "    if i != fold_test:\n",
    "        X_temp, y_temp = load_fold_data(i, files)\n",
    "        X_train.append(X_temp)\n",
    "        y_train.append(y_temp)\n",
    "\n",
    "# Concatenate the training data\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "# Hyperparameters\n",
    "best_config = {\n",
    "    'hidden_units': [256, 128, 64],\n",
    "    'activations': ['relu', 'relu', 'relu'],\n",
    "    'dropout_rate': 0.3,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.0001,\n",
    "    'regularization_type': None,\n",
    "    'regularization_value': 0.01\n",
    "}\n",
    "\n",
    "# Initialize and train the model\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=10,  # Classes from 0 to 9\n",
    "    hidden_units=best_config['hidden_units'],\n",
    "    dropout_rate=best_config['dropout_rate'],\n",
    "    activations=best_config['activations']\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_config['learning_rate']),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Example validation split\n",
    "X_val, y_val = X_train[:len(X_train)//10], y_train[:len(y_train)//10]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=best_config['batch_size'],\n",
    "    epochs=best_config['epochs']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c396a-254a-4c64-a7e7-3d08f28cf4f7",
   "metadata": {},
   "source": [
    "#### DeepFool Implementation\n",
    "[[go back to the top]](#deepfool)\n",
    "\n",
    "In this section we finally implement the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764eef8-2885-43a4-9dd9-425c1c9674aa",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "534a0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_units': [256, 128, 64],\n",
    "    'activations': ['relu', 'relu', 'relu'],\n",
    "    'dropout_rate': 0.3,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.0001,\n",
    "    'regularization_type': None,\n",
    "    'regularization_value': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc68905-703a-460c-abae-b63c54a2f312",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "[[go back to the top]](#deepfool)\n",
    "\n",
    "Here, we simply build and compile an MLP model based on the provided configuration as explained in the python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "253acf75-5348-40b7-8e8e-a6fd1ec4d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(input_dim, best_config):\n",
    "    \"\"\"\n",
    "    Build and compile an MLP model based on the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        best_config (dict): Dictionary containing hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled MLP model.\n",
    "    \"\"\"\n",
    "    model = MLP(\n",
    "                input_dim=X_train.shape[1],\n",
    "                output_dim=10,\n",
    "                hidden_units=best_config['hidden_units'],\n",
    "                dropout_rate=best_config['dropout_rate'],\n",
    "                activations=best_config['activations'],\n",
    "                regularization_type=best_config.get('regularization_type', None),\n",
    "                regularization_value=best_config.get('regularization_value', 0.01)\n",
    "            )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=best_config['learning_rate']),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a97b8b-cf1d-4781-9f22-cd4494fea352",
   "metadata": {},
   "source": [
    "#### Loading the datasets\n",
    "[[go back to the top]](#deepfool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "412a6313-52b8-4aa1-a076-044b13305b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all folds into a list\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "folds = [pd.read_csv(file) for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50f266-af6f-4090-bdc2-f0501c0c8af0",
   "metadata": {},
   "source": [
    "#### Adversarial Training and Evaluation with DeepFool Implementation\n",
    "[[go back to the top]](#deepfool)\n",
    "\n",
    "This code trains an MLP model using predefined configurations, including **regularization** and **early stopping** for optimization. The `deepfool_mlp` function is introduced to generate adversarial examples by iteratively **perturbing input data to mislead the model**. This setup enables both robust model training and the exploration of adversarial attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5ba3c-2571-4cbe-a527-929d2a8a4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fold_data(fold_number, files):\n",
    "    data = pd.read_csv(files[fold_number - 1]) \n",
    "    if data.empty:\n",
    "        print(f\"Erro: O arquivo {files[fold_number - 1]} está vazio ou não foi carregado corretamente.\")\n",
    "    labels = data.pop('Label').values\n",
    "    features = data.values\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def train_evaluate_model(config, X_train, y_train, X_val, y_val):\n",
    "    model = MLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10,\n",
    "        hidden_units=config['hidden_units'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        activations=config['activations'],\n",
    "        regularization_type=config.get('regularization_type', None),\n",
    "        regularization_value=config.get('regularization_value', 0.01)\n",
    "    )\n",
    "    \n",
    "    def loss_with_regularization(y_true, y_pred):\n",
    "        base_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        regularization_loss = model.compute_regularization_loss()\n",
    "        return base_loss + regularization_loss\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss=loss_with_regularization,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    return history \n",
    "\n",
    "def deepfool_mlp(model, x0, y0, max_iter=50, epsilon=1e-6):\n",
    "    x_adv = tf.identity(x0)  # Tensor inicial\n",
    "    for i in range(max_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            logits = model(x_adv)\n",
    "            pred_label = tf.argmax(logits, axis=-1).numpy()[0]\n",
    "\n",
    "        if pred_label != y0:\n",
    "            return x_adv.numpy()  # Retorna o adversarial se o modelo errar\n",
    "\n",
    "        gradients = tape.gradient(logits, x_adv).numpy()[0]\n",
    "        logits = logits.numpy()[0]\n",
    "        current_label = pred_label\n",
    "\n",
    "        perturbations = []\n",
    "        for k in range(len(logits)):\n",
    "            if k != y0:\n",
    "                diff_grad = gradients[k] - gradients[y0]\n",
    "                diff_logits = logits[k] - logits[y0]\n",
    "                perturbation = abs(diff_logits) / (np.linalg.norm(diff_grad) + epsilon)\n",
    "                perturbations.append((perturbation, k))\n",
    "\n",
    "        perturbations.sort(key=lambda x: x[0])  # Ordena pelo menor deslocamento necessário\n",
    "        r_min, _ = perturbations[0]\n",
    "        x_adv += r_min * gradients\n",
    "\n",
    "    return x_adv.numpy()  # Retorna a última iteração como fallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6f431-7207-42bd-b2ed-9663c132295d",
   "metadata": {},
   "source": [
    "#### Cross-Validation with Adversarial Robustness Assessment using DeepFool\n",
    "[[go back to the top]](#deepfool)\n",
    "\n",
    "This code performs 10-fold cross-validation on the MLP model, evaluating its performance on **accuracy**, **loss**, and **robustness** against adversarial attacks generated by DeepFool. For each fold, the model is trained on a subset of the data, validated on another, and tested on the remaining fold. Robustness is measured by the **model's ability** to resist adversarial examples crafted for each test sample. The final outputs are the average accuracy, loss, and robustness across all folds, providing a comprehensive evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7787acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy=0.6804, Loss=1.1354, Robustness=0.5945\n",
      "Fold 2: Accuracy=0.5439, Loss=1.2987, Robustness=0.4899\n",
      "Fold 3: Accuracy=0.6032, Loss=1.1992, Robustness=0.5178\n",
      "Fold 4: Accuracy=0.5808, Loss=1.2251, Robustness=0.4980\n",
      "Fold 5: Accuracy=0.6154, Loss=1.0490, Robustness=0.5331\n",
      "Fold 6: Accuracy=0.5176, Loss=1.5814, Robustness=0.4520\n",
      "Fold 7: Accuracy=0.5525, Loss=1.3745, Robustness=0.5060\n",
      "Fold 8: Accuracy=0.6154, Loss=1.2348, Robustness=0.5186\n",
      "Fold 9: Accuracy=0.5417, Loss=1.3398, Robustness=0.4608\n",
      "Fold 10: Accuracy=0.6045, Loss=1.1937, Robustness=0.5293\n",
      "\n",
      "Cross-Validation Summary:\n",
      "Average Accuracy: 0.5855\n",
      "Average Loss: 1.2632\n",
      "Average Robustness: 0.5100\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mlp_deepfool(datasets, model_builder, params):\n",
    "    folds = list(range(1, 11))\n",
    "    accuracy_values = []\n",
    "    loss_values = []\n",
    "    robustness_values = []\n",
    "\n",
    "    for test_fold in folds:\n",
    "        train_val_folds = [fold for fold in folds if fold != test_fold]\n",
    "        train_folds = train_val_folds[:-1]\n",
    "        val_fold = train_val_folds[-1]\n",
    "        \n",
    "        # Load data\n",
    "        X_train, y_train = [], []\n",
    "        for fold in train_folds:\n",
    "            X_temp, y_temp = load_fold_data(fold, files)\n",
    "            X_train.append(X_temp)\n",
    "            y_train.append(y_temp)\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "        X_val, y_val = load_fold_data(val_fold, files)\n",
    "        X_test, y_test = load_fold_data(test_fold, files)\n",
    "\n",
    "        # Train model\n",
    "        model = model_builder(input_dim=X_train.shape[1], best_config=best_config)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['epochs'],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Evaluate performance\n",
    "        fold_loss, fold_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        accuracy_values.append(fold_accuracy)\n",
    "        loss_values.append(fold_loss)\n",
    "\n",
    "        # Robustness with DeepFool\n",
    "        adversarial_success = 0\n",
    "        for idx in range(len(X_test)):\n",
    "            x0 = np.expand_dims(X_test[idx], axis=0)\n",
    "            y0 = y_test[idx]\n",
    "            x_adv = deepfool_mlp(model, x0, y0)\n",
    "            adv_pred = tf.argmax(model(x_adv), axis=-1).numpy()[0]\n",
    "\n",
    "            if adv_pred != y0:\n",
    "                adversarial_success += 1\n",
    "\n",
    "        robustness = 1 - (adversarial_success / len(X_test))\n",
    "        robustness_values.append(robustness)\n",
    "        print(f\"Fold {test_fold}: Accuracy={fold_accuracy:.4f}, Loss={fold_loss:.4f}, Robustness={robustness:.4f}\")\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_accuracy = np.mean(accuracy_values)\n",
    "    avg_loss = np.mean(loss_values)\n",
    "    avg_robustness = np.mean(robustness_values)\n",
    "\n",
    "    # Display averages\n",
    "    print(\"\\nCross-Validation Summary:\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average Robustness: {avg_robustness:.4f}\")\n",
    "\n",
    "    return accuracy_values, loss_values, robustness_values\n",
    "\n",
    "accuracy_values, loss_values, robustness_values = cross_validation_mlp_deepfool(\n",
    "    datasets=files,            \n",
    "    model_builder=build_mlp_model,  \n",
    "    params=best_config        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75c91e-ad99-41de-bfba-dd5d5818c35f",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "[[go back to the top]](#deepfool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba7109-00f5-40e1-b8ff-5015bf13c296",
   "metadata": {},
   "source": [
    "- The **cross-validation** results provide valuable insights into the performance of our MLP model. The average accuracy of **58.55%** indicates that the model performs moderately well at classifying the urban sound data. \n",
    "\n",
    "- The average loss of **1.2632** suggests that the model struggles to confidently separate some classes. \n",
    "\n",
    "- The robustness score, averaging **51.00%**, reveals that the model is somewhat vulnerable to adversarial attacks crafted by DeepFool.\n",
    "\n",
    "While the results aren't exactly what we hoped for, they show that there's plenty of room for **improvement**, and that's something we're excited to keep working on.\n",
    "\n",
    "Looking back, this project came with its fair share of challenges, especially when it came to feature extraction and modeling. But overall, it was a really rewarding experience. We gained a much deeper understanding of how neural networks work and how different hyperparameters (such as activation functions, epochs, etc...) can really affect the outcome. It’s been a great learning process, and we’re looking forward to building on what we've learned to improve the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4473b-a0d0-4f0f-90dc-0ce503236dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
