{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptum (MLP) <a name=\"multilayer\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use **Multilayer Perceptron (MLP)** because it is a flexible neural network architecture. MLPs are great for solving **classification problems**\n",
    "\n",
    "For this model we will define the model architecture and the training strategy consisting in:\n",
    "- **Number of layers**\n",
    "- **Number of neurons of each layer**\n",
    "- **Choice of the activation functions**\n",
    "- **Optimizer** \n",
    "- **Learning hyperparameters** (e.g., learning rate, mini-batch size, number of epochs, etc.)\n",
    "- **Regularization techniques to adopt** (e.g., early stopping, weight regularization, dropout)\n",
    "\n",
    "The network works by processing data through **multiple layers**, with each layer learning to capture different features of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture definition and Training Strategy <a name=\"archi-train\"></a>\n",
    "[[go back to the top]](#multilayer)\n",
    "\n",
    "For the architecture of our MLP model we need, as mentioned above, the number of layers, neurons, and choose the activation functions such as relu, softmax and Tanh for example.\n",
    "\n",
    "We used **dictionaries** to organize and store different options for **hyperparameters**. This allows us to easily experiment with different configurations and manage the settings efficiently.\n",
    "\n",
    "To optimize our model, we decided to do a grid search to **update and select the best hyperparameter combination in the first iteration**, for one epoch only in order to be less complex. This means that in the beginning, we test several combinations of hyperparameters to find the one that performs best. By doing this, we can quickly narrow down the best model for our task, improving the **accuracy** of the predictions.\n",
    "\n",
    "Additionally, we will use the **ADAM** optimizer, which is a popular choice for training neural networks due to its adaptive learning rate and efficient performance.\n",
    "We also implemented **early stopping** to prevent overfitting by monitoring the model's performance and halting training when it stops improving.\n",
    "\n",
    "In this way, the process of **testing and updating** in the first iteration helps us fine-tune the model efficiently, and **selecting the best combination** ensures we are using the most effective settings for our dataset.\n",
    "\n",
    "The following table defines the possible combinations of hyperparameters we tested:\n",
    "\n",
    "| <span style=\"color: #C70039;\">**Hyperparameter**</span> | <span style=\"color: #C70039;\">**Options**</span>        |\n",
    "|-----------------------------------------------------|-------------------------------------------------------|\n",
    "| <span style=\"color: #00bfae;\">**Hidden Units**</span> | [128, 64, 32], [256, 128, 64], [256, 128, 64, 32]                |\n",
    "| <span style=\"color: #00bfae;\">**Activation Functions**</span> | reLU, sigmoid, tanh                             |\n",
    "| <span style=\"color: #00bfae;\">**Dropout Rate**</span> | 0.3, 0.5                                          |\n",
    "| <span style=\"color: #00bfae;\">**Batch Size**</span>   | 32                                               |\n",
    "| <span style=\"color: #00bfae;\">**Epochs**</span>       | 1                                                 |\n",
    "| <span style=\"color: #00bfae;\">**Regularizations**</span>       | None, L1 (Lasso), L2 (Ridge)                                            |\n",
    "| <span style=\"color: #00bfae;\">**Learning Rate**</span> | 0.001, 0.0001                                     |\n",
    "\n",
    "\n",
    "<span style=\"color: #C70039;\">**Note:**</span>\n",
    "- <span style=\"color: #00bfae;\">**Hidden Units**</span> consists in the number of layers and the number of each neurons of each layer, for example in this case [256, 128, 64], it defines 3 layers with 256, 128 and 64 neurons, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from pathos.multiprocessing import Pool\n",
    "import random \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units, dropout_rate, activations, regularization_type=None, regularization_value=0.01):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = []\n",
    "        self.regularization_type = regularization_type\n",
    "        self.regularization_value = regularization_value\n",
    "\n",
    "        # Construção das camadas ocultas\n",
    "        for units, activation in zip(hidden_units, activations):\n",
    "            self.hidden_layers.append(\n",
    "                tf.keras.layers.Dense(units, activation=activation)\n",
    "            )\n",
    "            self.hidden_layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, activation='softmax')  # Classificação multi-classe\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def compute_regularization_loss(self):\n",
    "        regularization_loss = 0.0\n",
    "        if self.regularization_type:\n",
    "            for layer in self.hidden_layers:\n",
    "                if isinstance(layer, tf.keras.layers.Dense):\n",
    "                    weights = layer.kernel\n",
    "                    if self.regularization_type == 'l1':\n",
    "                        regularization_loss += tf.reduce_sum(tf.abs(weights)) * self.regularization_value\n",
    "                    elif self.regularization_type == 'l2':\n",
    "                        regularization_loss += tf.reduce_sum(tf.square(weights)) * self.regularization_value\n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_configs(configurations):\n",
    "    keys, values = zip(*configurations.items())\n",
    "    return [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# Função para carregar os dados de um fold específico\n",
    "def load_fold_data(fold_number, files):\n",
    "    data = pd.read_csv(files[fold_number])\n",
    "    labels = data.pop('Label').values\n",
    "    features = data.values\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration: {'hidden_units': [256, 128, 64], 'activations': ['relu', 'relu', 'relu'], 'dropout_rate': 0.3, 'batch_size': 64, 'epochs': 20, 'learning_rate': 0.0001, 'regularization_type': None, 'regularization_value': 0.01}, Best accuracy: 0.7216494679450989\n"
     ]
    }
   ],
   "source": [
    "# Treinar e avaliar o modelo\n",
    "def train_evaluate_model(config, X_train, y_train, X_val, y_val):\n",
    "    model = MLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10,\n",
    "        hidden_units=config['hidden_units'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        activations=config['activations'],\n",
    "        regularization_type=config.get('regularization_type', None),\n",
    "        regularization_value=config.get('regularization_value', 0.01)\n",
    "    )\n",
    "    \n",
    "    # Função de perda com regularização\n",
    "    def loss_with_regularization(y_true, y_pred):\n",
    "        base_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        regularization_loss = model.compute_regularization_loss()\n",
    "        return base_loss + regularization_loss\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss=loss_with_regularization,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return max(history.history['val_accuracy'])\n",
    "\n",
    "\n",
    "## Cross-validation, apenas uma iteração (1 fold)\n",
    "def cross_validate_model(config, files):\n",
    "    # Apenas o primeiro fold para validação\n",
    "    fold_number = 0\n",
    "    X_val, y_val = load_fold_data(fold_number, files)\n",
    "    X_train, y_train = [], []\n",
    "    \n",
    "    # Treino com os outros folds\n",
    "    for i in range(len(files)):\n",
    "        if i != fold_number:\n",
    "            X_temp, y_temp = load_fold_data(i, files)\n",
    "            X_train.append(X_temp)\n",
    "            y_train.append(y_temp)\n",
    "    \n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    \n",
    "    # Treinar e avaliar para este fold\n",
    "    accuracy = train_evaluate_model(config, X_train, y_train, X_val, y_val)\n",
    "    return accuracy  # Retorna a acurácia deste único fold\n",
    "\n",
    "# Função para avaliação em paralelo\n",
    "def evaluate_config_parallel(args):\n",
    "    config, files = args\n",
    "    accuracy = cross_validate_model(config, files)\n",
    "    \n",
    "    # Salvar os resultados no arquivo\n",
    "    with open(\"results_log.txt\", \"a\") as f:\n",
    "        f.write(f\"Configuration: {config} | Accuracy: {accuracy}\\n\")\n",
    "    \n",
    "    return config, accuracy\n",
    "\n",
    "def generate_activation_combinations(hidden_units_list, activations_list = ['relu', 'relu', 'relu'], num_combinations=1):\n",
    "    activation_combinations = []\n",
    "    for hidden_units in hidden_units_list:\n",
    "        layers = len(hidden_units)\n",
    "        # Gerar combinações aleatórias de ativações para este número de camadas\n",
    "        for _ in range(num_combinations):\n",
    "            random_combination = [random.choice(activations_list) for _ in range(layers)]\n",
    "            activation_combinations.append((hidden_units, random_combination))  # Retorna como tupla\n",
    "    return activation_combinations\n",
    "\n",
    "\n",
    "hidden_units_list = [[128, 64, 32], [256, 128, 64]]\n",
    "\n",
    "# Gerar combinações de hidden_units e activations\n",
    "activation_combinations = generate_activation_combinations(hidden_units_list)\n",
    "\n",
    "# Separar hidden_units e activations em listas distintas\n",
    "hidden_units = [combo[0] for combo in activation_combinations]\n",
    "activations = [combo[1] for combo in activation_combinations]\n",
    "\n",
    "# Definições de hiperparâmetros corrigidas\n",
    "configurations = {\n",
    "    \"hidden_units\": hidden_units,  # Somente os hidden_units\n",
    "    \"activations\": activations,    # Somente as ativações correspondentes\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4],\n",
    "    \"batch_size\": [32,64],\n",
    "    \"epochs\": [20,50],\n",
    "    \"learning_rate\": [0.001, 0.0001],\n",
    "    \"regularization_type\": [None, 'l1', 'l2'],\n",
    "    \"regularization_value\": [0.01, 0.001],\n",
    "}\n",
    "\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1,11)] \n",
    "\n",
    "all_configs = generate_configs(configurations)\n",
    " \n",
    "all_configs = random.sample(all_configs, k=300)  # Exemplo: selecionar 100 aleatoriamente\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    if os.path.exists(\"results_log.txt\"):\n",
    "        os.remove(\"results_log.txt\")\n",
    "    if os.path.exists(\"erros.txt\"):\n",
    "        os.remove(\"erros.txt\")\n",
    "    \n",
    "    num_workers = 8\n",
    "    with Pool(num_workers) as pool:\n",
    "        results = pool.map(evaluate_config_parallel, [(config, files) for config in all_configs])\n",
    "\n",
    "    # Encontrar a melhor configuração\n",
    "    best_config, best_accuracy = max(results, key=lambda x: x[1])\n",
    "    print(f\"Best configuration: {best_config}, Best accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the optimal hyperparameter configuration identified: **'hidden_units':** [256, 128, 64], **'activations':** ['relu', 'relu', 'relu'], **'dropout_rate':** 0.3, **'batch_size':** 64, **'epochs':** 20, **'learning_rate':** 0.0001, **'regularization_type':** None, **'regularization_value':** 0.01** with a **best accuracy** of 0.7216, we will now implement data augmentation techniques to evaluate and enhance model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data saved to augmented_datasets/urbansounds_features_fold1.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold2.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold3.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold4.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold5.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold6.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold7.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold8.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold9.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold10.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Pitching\n",
    "# Funções de data augmentation\n",
    "def add_noise(features, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level, features.shape)\n",
    "    return features + noise\n",
    "\n",
    "def scale_features(features, scale_range=(0.9, 1.1)):\n",
    "    scale_factor = np.random.uniform(scale_range[0], scale_range[1], features.shape)\n",
    "    return features * scale_factor\n",
    "\n",
    "def augment_data(features, labels, augmentation_count=2):\n",
    "    augmented_features = []\n",
    "    augmented_labels = []\n",
    "    for _ in range(augmentation_count):\n",
    "        augmented_features.append(add_noise(features))\n",
    "        augmented_features.append(scale_features(features))\n",
    "        augmented_labels.extend(labels)\n",
    "        augmented_labels.extend(labels)\n",
    "    return np.vstack(augmented_features), np.array(augmented_labels)\n",
    "\n",
    "# Processar os arquivos CSVs\n",
    "def augment_csv_files(files, output_dir=\"augmented_datasets\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for file in files:\n",
    "        data = pd.read_csv(file)\n",
    "        labels = data.pop('Label').values\n",
    "        features = data.values\n",
    "        \n",
    "        # Realizar data augmentation\n",
    "        aug_features, aug_labels = augment_data(features, labels, augmentation_count=2)\n",
    "        \n",
    "        # Combinar os dados originais com os aumentados\n",
    "        combined_features = np.vstack((features, aug_features))\n",
    "        combined_labels = np.hstack((labels, aug_labels))\n",
    "        \n",
    "        # Salvar o novo CSV\n",
    "        combined_data = pd.DataFrame(combined_features, columns=data.columns)\n",
    "        combined_data['Label'] = combined_labels\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file))\n",
    "        combined_data.to_csv(output_file, index=False)\n",
    "        print(f\"Augmented data saved to {output_file}\")\n",
    "\n",
    "# Aplicar a função nos arquivos originais\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "augment_csv_files(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10: Accuracy = 0.6706\n",
      "Fold 2/10: Accuracy = 0.6036\n",
      "Fold 3/10: Accuracy = 0.6045\n",
      "Fold 4/10: Accuracy = 0.6097\n",
      "Fold 5/10: Accuracy = 0.6799\n",
      "Fold 6/10: Accuracy = 0.5548\n",
      "Fold 7/10: Accuracy = 0.5804\n",
      "Fold 8/10: Accuracy = 0.6476\n",
      "Fold 9/10: Accuracy = 0.6172\n",
      "Fold 10/10: Accuracy = 0.6562\n",
      "\n",
      "Cross-Validation Results: Mean Accuracy = 0.6224, Std Dev = 0.0382\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Função para executar validação cruzada com a melhor configuração\n",
    "def cross_validate_best_config(config, files):\n",
    "    accuracies = []\n",
    "    \n",
    "    # Para cada fold, use-o como validação e os demais como treino\n",
    "    for fold_number in range(len(files)):\n",
    "        # Carregar o fold atual como validação\n",
    "        X_val, y_val = load_fold_data(fold_number, files)\n",
    "        X_train, y_train = [], []\n",
    "        \n",
    "        # Combinar os outros folds como treino\n",
    "        for i in range(len(files)):\n",
    "            if i != fold_number:\n",
    "                X_temp, y_temp = load_fold_data(i, files)\n",
    "                X_train.append(X_temp)\n",
    "                y_train.append(y_temp)\n",
    "        \n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        \n",
    "        # Treinar e avaliar para este fold\n",
    "        accuracy = train_evaluate_model(config, X_train, y_train, X_val, y_val)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Fold {fold_number + 1}/{len(files)}: Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    # Calcular média e desvio-padrão\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    print(f\"\\nCross-Validation Results: Mean Accuracy = {mean_accuracy:.4f}, Std Dev = {std_accuracy:.4f}\")\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Melhor configuração encontrada\n",
    "best_config = {\n",
    "    \"hidden_units\": [256, 128, 64],\n",
    "    \"activations\": ['relu', 'relu', 'relu'],\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"regularization_type\": None,\n",
    "    \"regularization_value\": 0.01\n",
    "}\n",
    "\n",
    "# Lista de arquivos dos folds (os dados aumentados também podem ser usados aqui)\n",
    "files = [f'augmented_datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Rodar a validação cruzada com a melhor configuração\n",
    "    mean_accuracy, std_accuracy = cross_validate_best_config(best_config, files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Fold: 1, Validation Fold: 2, Test Accuracy: 0.5990990996360779\n",
      "Test Fold: 1, Validation Fold: 3, Test Accuracy: 0.5495495200157166\n",
      "Test Fold: 1, Validation Fold: 4, Test Accuracy: 0.537162184715271\n",
      "Test Fold: 1, Validation Fold: 5, Test Accuracy: 0.6036036014556885\n",
      "Test Fold: 1, Validation Fold: 6, Test Accuracy: 0.5213963985443115\n",
      "Test Fold: 1, Validation Fold: 7, Test Accuracy: 0.5945945978164673\n",
      "Test Fold: 1, Validation Fold: 8, Test Accuracy: 0.5765765905380249\n",
      "Test Fold: 1, Validation Fold: 9, Test Accuracy: 0.5990990996360779\n",
      "Test Fold: 2, Validation Fold: 1, Test Accuracy: 0.5448648929595947\n",
      "Test Fold: 2, Validation Fold: 3, Test Accuracy: 0.5372973084449768\n",
      "Test Fold: 2, Validation Fold: 4, Test Accuracy: 0.5751351118087769\n",
      "Test Fold: 2, Validation Fold: 5, Test Accuracy: 0.5481081008911133\n",
      "Test Fold: 2, Validation Fold: 6, Test Accuracy: 0.5243242979049683\n",
      "Test Fold: 2, Validation Fold: 7, Test Accuracy: 0.4972972869873047\n",
      "Test Fold: 2, Validation Fold: 8, Test Accuracy: 0.5297297239303589\n",
      "Test Fold: 2, Validation Fold: 9, Test Accuracy: 0.5048648715019226\n",
      "Test Fold: 3, Validation Fold: 1, Test Accuracy: 0.5747475028038025\n",
      "Test Fold: 3, Validation Fold: 2, Test Accuracy: 0.5434343218803406\n",
      "Test Fold: 3, Validation Fold: 4, Test Accuracy: 0.5020201802253723\n",
      "Test Fold: 3, Validation Fold: 5, Test Accuracy: 0.5626262426376343\n",
      "Test Fold: 3, Validation Fold: 6, Test Accuracy: 0.5696969628334045\n",
      "Test Fold: 3, Validation Fold: 7, Test Accuracy: 0.5747475028038025\n",
      "Test Fold: 3, Validation Fold: 8, Test Accuracy: 0.5464646220207214\n",
      "Test Fold: 3, Validation Fold: 9, Test Accuracy: 0.5727272629737854\n",
      "Test Fold: 4, Validation Fold: 1, Test Accuracy: 0.6623931527137756\n",
      "Test Fold: 4, Validation Fold: 2, Test Accuracy: 0.622863233089447\n",
      "Test Fold: 4, Validation Fold: 3, Test Accuracy: 0.6752136945724487\n",
      "Test Fold: 4, Validation Fold: 5, Test Accuracy: 0.5833333134651184\n",
      "Test Fold: 4, Validation Fold: 6, Test Accuracy: 0.6004273295402527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 17:11:10.031142: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [64] vs. [0]\n",
      "\t [[{{function_node __inference_one_step_on_data_7025009}}{{node adam/truediv_7}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node adam/truediv_7 defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 725, in start\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 215, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n\n  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_57880/3902091085.py\", line 85, in <module>\n\n  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_57880/3902091085.py\", line 43, in cross_validate_with_test\n\n  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_57880/3902091085.py\", line 75, in evaluate_on_test\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 73, in train_step\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/base_optimizer.py\", line 344, in apply_gradients\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/base_optimizer.py\", line 409, in apply\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/base_optimizer.py\", line 472, in _backend_apply_gradients\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 122, in _backend_update_step\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 136, in _distributed_tf_update_step\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 133, in apply_grad_to_update_var\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/adam.py\", line 147, in update_step\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/ops/numpy.py\", line 5876, in divide\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/sparse.py\", line 780, in sparse_wrapper\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/numpy.py\", line 2316, in divide\n\nIncompatible shapes: [64] vs. [0]\n\t [[{{node adam/truediv_7}}]] [Op:__inference_one_step_on_iterator_7025062]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/urbansounds_features_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)]\n\u001b[1;32m     83\u001b[0m augmented_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugmented_datasets/urbansounds_features_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)]\n\u001b[0;32m---> 85\u001b[0m mean_accuracy, std_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate_with_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Mean Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Final Std Dev: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 43\u001b[0m, in \u001b[0;36mcross_validate_with_test\u001b[0;34m(files, augmented_files, best_config)\u001b[0m\n\u001b[1;32m     40\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m train_evaluate_model(best_config, X_train, y_train, X_val, y_val)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Avaliar no conjunto de teste\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_on_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m test_accuracies\u001b[38;5;241m.\u001b[39mappend(test_accuracy)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Fold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_fold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Fold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_fold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 75\u001b[0m, in \u001b[0;36mevaluate_on_test\u001b[0;34m(config, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(\n\u001b[1;32m     60\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     61\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     regularization_value\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregularization_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     70\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     71\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[1;32m     72\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 75\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_loss, test_accuracy\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node adam/truediv_7 defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 725, in start\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 215, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n\n  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_57880/3902091085.py\", line 85, in <module>\n\n  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_57880/3902091085.py\", line 43, in cross_validate_with_test\n\n  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_57880/3902091085.py\", line 75, in evaluate_on_test\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 73, in train_step\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/base_optimizer.py\", line 344, in apply_gradients\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/base_optimizer.py\", line 409, in apply\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/base_optimizer.py\", line 472, in _backend_apply_gradients\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 122, in _backend_update_step\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 136, in _distributed_tf_update_step\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 133, in apply_grad_to_update_var\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/adam.py\", line 147, in update_step\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/ops/numpy.py\", line 5876, in divide\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/sparse.py\", line 780, in sparse_wrapper\n\n  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/numpy.py\", line 2316, in divide\n\nIncompatible shapes: [64] vs. [0]\n\t [[{{node adam/truediv_7}}]] [Op:__inference_one_step_on_iterator_7025062]"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def load_augmented_data(fold_number, augmented_files):\n",
    "    # Função para carregar dados aumentados\n",
    "    return load_fold_data(fold_number, augmented_files)\n",
    "\n",
    "def cross_validate_with_test(files, augmented_files, best_config):\n",
    "    \"\"\"\n",
    "    Executa a validação cruzada 10-fold com o seguinte esquema:\n",
    "    - 1 fold para teste (dados originais).\n",
    "    - 1 fold para validação (dados originais).\n",
    "    - 8 folds para treinamento (dados aumentados).\n",
    "    \"\"\"\n",
    "    # Combinações de folds: [(test_fold, val_fold)]\n",
    "    folds = list(range(1,len(files)))\n",
    "    combinations = [(test, val) for test, val in itertools.permutations(folds, 2)]\n",
    "    \n",
    "    test_accuracies = []\n",
    "\n",
    "    for test_fold, val_fold in combinations:\n",
    "        # Carregar dados de teste (originais)\n",
    "        X_test, y_test = load_fold_data(test_fold, files)\n",
    "\n",
    "        # Carregar dados de validação (originais)\n",
    "        X_val, y_val = load_fold_data(val_fold, files)\n",
    "\n",
    "        # Carregar dados de treinamento (aumentados)\n",
    "        train_folds = [i for i in folds if i != test_fold and i != val_fold]\n",
    "        X_train, y_train = [], []\n",
    "        for fold in train_folds:\n",
    "            X_temp, y_temp = load_augmented_data(fold, augmented_files)\n",
    "            X_train.append(X_temp)\n",
    "            y_train.append(y_temp)\n",
    "        \n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "        # Treinar e avaliar o modelo\n",
    "        accuracy = train_evaluate_model(best_config, X_train, y_train, X_val, y_val)\n",
    "\n",
    "        # Avaliar no conjunto de teste\n",
    "        test_loss, test_accuracy = evaluate_on_test(best_config, X_train, y_train, X_test, y_test)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f\"Test Fold: {test_fold}, Validation Fold: {val_fold}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Cálculo da média e desvio padrão da acurácia no teste\n",
    "    mean_accuracy = np.mean(test_accuracies)\n",
    "    std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "    print(f\"Mean Test Accuracy: {mean_accuracy}, Std Dev: {std_accuracy}\")\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "def evaluate_on_test(config, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Treina o modelo com o conjunto completo de treinamento e avalia no conjunto de teste.\n",
    "    \"\"\"\n",
    "    model = MLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10,\n",
    "        hidden_units=config['hidden_units'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        activations=config['activations'],\n",
    "        regularization_type=config.get('regularization_type', None),\n",
    "        regularization_value=config.get('regularization_value', 0.01)\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=config['batch_size'], epochs=config['epochs'], verbose=0)\n",
    "    \n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "    augmented_files = [f'augmented_datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "    \n",
    "    mean_accuracy, std_accuracy = cross_validate_with_test(files, augmented_files, best_config)\n",
    "    print(f\"Final Mean Accuracy: {mean_accuracy}, Final Std Dev: {std_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
