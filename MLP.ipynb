{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptum (MLP) <a name=\"multilayer\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use **Multilayer Perceptron (MLP)** because it is a flexible neural network architecture. MLPs are great for solving **classification problems**\n",
    "\n",
    "For this model we will define the model architecture and the training strategy consisting in:\n",
    "- **Number of layers**\n",
    "- **Number of neurons of each layer**\n",
    "- **Choice of the activation functions**\n",
    "- **Optimizer** \n",
    "- **Learning hyperparameters** (e.g., learning rate, mini-batch size, number of epochs, etc.)\n",
    "- **Regularization techniques to adopt** (e.g., early stopping, weight regularization, dropout)\n",
    "\n",
    "The network works by processing data through **multiple layers**, with each layer learning to capture different features of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture definition and Training Strategy <a name=\"archi-train\"></a>\n",
    "\n",
    "For the architecture of our MLP model we need, as mentioned above, the number of layers, neurons, and choose the activation functions such as relu, softmax and Tanh for example.\n",
    "\n",
    "We used **dictionaries** to organize and store different options for **hyperparameters**. This allows us to easily experiment with different configurations and manage the settings efficiently.\n",
    "\n",
    "To optimize our model, we decided to do a grid search to **update and select the best hyperparameter combination in the first iteration**. This means that in the beginning, we test several combinations of hyperparameters to find the one that performs best. By doing this, we can quickly narrow down the best model for our task, improving the **accuracy** of the predictions.\n",
    "\n",
    "Additionally, we will use the **ADAM** optimizer, which is a popular choice for training neural networks due to its adaptive learning rate and efficient performance.\n",
    "We also implemented **early stopping** to prevent overfitting by monitoring the model's performance and halting training when it stops improving.\n",
    "\n",
    "In this way, the process of **testing and updating** in the first iteration helps us fine-tune the model efficiently, and **selecting the best combination** ensures we are using the most effective settings for our dataset.\n",
    "\n",
    "The following table defines the possible combinations of hyperparameters we tested:\n",
    "\n",
    "| <span style=\"color: #C70039;\">**Hyperparameter**</span> | <span style=\"color: #C70039;\">**Options**</span>        |\n",
    "|-----------------------------------------------------|-------------------------------------------------------|\n",
    "| <span style=\"color: #00bfae;\">**Hidden Units**</span> | [128, 64, 32], [256, 128, 64], [256, 128, 64, 32]                |\n",
    "| <span style=\"color: #00bfae;\">**Activation Functions**</span> | reLU, sigmoid, tanh                             |\n",
    "| <span style=\"color: #00bfae;\">**Dropout Rate**</span> | 0.2, 0.3, 0.4                                         |\n",
    "| <span style=\"color: #00bfae;\">**Batch Size**</span>   | 32, 64                                               |\n",
    "| <span style=\"color: #00bfae;\">**Epochs**</span>       | 20, 50                                                 |\n",
    "| <span style=\"color: #00bfae;\">**Regularizations**</span>       | None, L1 (Lasso), L2 (Ridge)                                            |\n",
    "| <span style=\"color: #00bfae;\">**Learning Rate**</span> | 0.001, 0.0001                                     |\n",
    "\n",
    "\n",
    "<span style=\"color: #C70039;\">**Note:**</span>\n",
    "- <span style=\"color: #00bfae;\">**Hidden Units**</span> consists in the number of layers and the number of each neurons of each layer, for example in this case [256, 128, 64], it defines 3 layers with 256, 128 and 64 neurons, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Implementation  <a name=\"multilayer\"></a>\n",
    "[[go back to the top]](#multilayer)\n",
    "\n",
    "Here we start the MLP implementation with the provided explanation of our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from pathos.multiprocessing import Pool\n",
    "import random \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class MLP\n",
    "[[go back to the topic]](#mlp-implementation)\n",
    "\n",
    "Defines a flexible Multi-Layer Perceptron (MLP) with configurable layers, dropout, activation functions, and optional **l1** or **l2** regularization. Includes a method to compute regularization loss for better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units, dropout_rate, activations, regularization_type=None, regularization_value=0.01):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = []\n",
    "        self.regularization_type = regularization_type\n",
    "        self.regularization_value = regularization_value\n",
    "\n",
    "        for units, activation in zip(hidden_units, activations):\n",
    "            self.hidden_layers.append(\n",
    "                tf.keras.layers.Dense(units, activation=activation)\n",
    "            )\n",
    "            self.hidden_layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, activation='softmax')  \n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def compute_regularization_loss(self):\n",
    "        regularization_loss = 0.0\n",
    "        if self.regularization_type:\n",
    "            for layer in self.hidden_layers:\n",
    "                if isinstance(layer, tf.keras.layers.Dense):\n",
    "                    weights = layer.kernel\n",
    "                    if self.regularization_type == 'l1':\n",
    "                        regularization_loss += tf.reduce_sum(tf.abs(weights)) * self.regularization_value\n",
    "                    elif self.regularization_type == 'l2':\n",
    "                        regularization_loss += tf.reduce_sum(tf.square(weights)) * self.regularization_value\n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Configuration Generator and Load Fold Data\n",
    "[[go back to the topic]](#mlp-implementation)\n",
    "\n",
    "Generates all possible combinations of hyperparameter configurations from a dictionary of options using Cartesian product. Loads features and labels from a specific fold's dataset file, separating the Label column from the feature matrix for training and evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_configs(configurations):\n",
    "    keys, values = zip(*configurations.items())\n",
    "    return [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "def load_fold_data(fold_number, files):\n",
    "    data = pd.read_csv(files[fold_number])\n",
    "    labels = data.pop('Label').values\n",
    "    features = data.values\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train, Evaluate, and Cross-Validate MLP\n",
    "[[go back to the topic]](#mlp-implementation)\n",
    "\n",
    "This code trains and evaluates the MLP model using a specified configuration and implements single-fold cross-validation. The training process includes a custom loss function that integrates a regularization term to improve generalization, uses early stopping to prevent overfitting, and calculates validation accuracy on the held-out fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(config, X_train, y_train, X_val, y_val):\n",
    "    model = MLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10,\n",
    "        hidden_units=config['hidden_units'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        activations=config['activations'],\n",
    "        regularization_type=config.get('regularization_type', None),\n",
    "        regularization_value=config.get('regularization_value', 0.01)\n",
    "    )\n",
    "    \n",
    "    def loss_with_regularization(y_true, y_pred):\n",
    "        base_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        regularization_loss = model.compute_regularization_loss()\n",
    "        return base_loss + regularization_loss\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss=loss_with_regularization,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return max(history.history['val_accuracy'])\n",
    "\n",
    "\n",
    "# Cross-validation, just one iteration (1 fold)\n",
    "def cross_validate_model(config, files):\n",
    "    # Only the first fold for validation\n",
    "    fold_number = 0\n",
    "    X_val, y_val = load_fold_data(fold_number, files)\n",
    "    X_train, y_train = [], []\n",
    "    \n",
    "    # Training with other folds\n",
    "    for i in range(len(files)):\n",
    "        if i != fold_number:\n",
    "            X_temp, y_temp = load_fold_data(i, files)\n",
    "            X_train.append(X_temp)\n",
    "            y_train.append(y_temp)\n",
    "    \n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    \n",
    "    # Train and evaluate for this fold\n",
    "    accuracy = train_evaluate_model(config, X_train, y_train, X_val, y_val)\n",
    "    return accuracy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parallel Evaluation and Activation Combination Generation\n",
    "[[go back to the topic]](#mlp-implementation)\n",
    "\n",
    "This code enables parallel evaluation of model configurations by performing cross-validation and logging the results (configuration and accuracy). It also generates random activation function combinations for different hidden layer structures, creating varied configurations for testing and optimization. The results are saved in a log file for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_config_parallel(args):\n",
    "    config, files = args\n",
    "    accuracy = cross_validate_model(config, files)\n",
    "    \n",
    "    with open(\"results_log.txt\", \"a\") as f:\n",
    "        f.write(f\"Configuration: {config} | Accuracy: {accuracy}\\n\")\n",
    "    \n",
    "    return config, accuracy\n",
    "\n",
    "def generate_activation_combinations(hidden_units_list, activations_list = ['relu', 'sigmoid', 'tanh'], num_combinations=1):\n",
    "    activation_combinations = []\n",
    "    for hidden_units in hidden_units_list:\n",
    "        layers = len(hidden_units)\n",
    "        for _ in range(num_combinations):\n",
    "            random_combination = [random.choice(activations_list) for _ in range(layers)]\n",
    "            activation_combinations.append((hidden_units, random_combination))  \n",
    "    return activation_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate and Select Hyperparameter Configurations\n",
    "[[go back to the topic]](#mlp-implementation)\n",
    "\n",
    "This code generates combinations of hidden units, activation functions, and other hyperparameters such as dropout rate, batch size, epochs, and regularization settings. It then creates a list of all possible configurations for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units_list = [[128, 64, 32], [256, 128, 64], [256, 128, 64, 32]]\n",
    "\n",
    "activation_combinations = generate_activation_combinations(hidden_units_list)\n",
    "\n",
    "hidden_units = [combo[0] for combo in activation_combinations]\n",
    "activations = [combo[1] for combo in activation_combinations]\n",
    "\n",
    "configurations = {\n",
    "    \"hidden_units\": hidden_units,  \n",
    "    \"activations\": activations,    \n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4],\n",
    "    \"batch_size\": [32,64],\n",
    "    \"epochs\": [20,50],\n",
    "    \"learning_rate\": [0.001, 0.0001],\n",
    "    \"regularization_type\": [None, 'l1', 'l2'],\n",
    "    \"regularization_value\": [0.01, 0.001],\n",
    "}\n",
    "\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1,11)] \n",
    "\n",
    "all_configs = generate_configs(configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Evaluation and Best Configuration Selection\n",
    "[[go back to the topic]](#mlp-implementation)\n",
    "\n",
    "This code evaluates different model configurations in parallel using multiple workers. It first removes any existing result files (**results_log.txt**) to ensure a clean start. Then, it uses a multiprocessing pool with 8 workers to evaluate each configuration in **all_configs**. The results are logged and the best configuration, based on accuracy, is selected and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration: {'hidden_units': [256, 128, 64], 'activations': ['relu', 'relu', 'relu'], 'dropout_rate': 0.3, 'batch_size': 64, 'epochs': 20, 'learning_rate': 0.0001, 'regularization_type': None, 'regularization_value': 0.01}, Best accuracy: 0.7216494679450989\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    if os.path.exists(\"results_log.txt\"):\n",
    "        os.remove(\"results_log.txt\")\n",
    "    num_workers = 8\n",
    "    with Pool(num_workers) as pool:\n",
    "        results = pool.map(evaluate_config_parallel, [(config, files) for config in all_configs])\n",
    "\n",
    "    # Find the best configuration\n",
    "    best_config, best_accuracy = max(results, key=lambda x: x[1])\n",
    "    print(f\"Best configuration: {best_config}, Best accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the optimal hyperparameter configuration identified: **'hidden_units':** [256, 128, 64], **'activations':** ['relu', 'relu', 'relu'], **'dropout_rate':** 0.3, **'batch_size':** 64, **'epochs':** 20, **'learning_rate':** 0.0001, **'regularization_type':** None, **'regularization_value':** 0.01** with a **best accuracy** of 0.7216, we will now implement data augmentation techniques to evaluate and enhance model robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "[[go back to the topic]](#multilayer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are applying data augmentation to enhance the diversity of our dataset. Techniques include **adding noise**, **scaling feature values**, **simulating \n",
    "pitch shifts**, and **equalizing frequency ranges**.\n",
    "\n",
    "These transformations aim to improve model **generalization** and **robustness**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(features, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level, features.shape)\n",
    "    return features + noise\n",
    "\n",
    "def scale_features(features, scale_range=(0.9, 1.1)):\n",
    "    scale_factor = np.random.uniform(scale_range[0], scale_range[1], features.shape)\n",
    "    return features * scale_factor\n",
    "\n",
    "def pitch_shift(features, shift_range=(-2, 2)):\n",
    "    \"\"\"\n",
    "    Aplica uma modificação simulada nos MFCCs para simular pitch shifting.\n",
    "    shift_range define o número de semitons.\n",
    "    \"\"\"\n",
    "    shift_value = np.random.uniform(*shift_range)  \n",
    "    shifted_features = features.copy()\n",
    "    if shifted_features.shape[1] >= 13:  \n",
    "        shifted_features[:, :13] += shift_value\n",
    "    return shifted_features\n",
    "\n",
    "def equalize(features, eq_factor=1.5):\n",
    "    \"\"\"\n",
    "    Simula equalização amplificando as frequências médias.\n",
    "    Amplificação arbitrária nas colunas centrais.\n",
    "    \"\"\"\n",
    "    features = np.copy(features)\n",
    "    mid_point = features.shape[1] // 2\n",
    "    if mid_point - 5 >= 0 and mid_point + 5 < features.shape[1]:\n",
    "        features[:, mid_point - 5:mid_point + 5] *= eq_factor\n",
    "    return features\n",
    "\n",
    "def augment_data(features, labels, augmentation_count=2):\n",
    "    augmented_features = []\n",
    "    augmented_labels = []\n",
    "    for _ in range(augmentation_count):\n",
    "        augmented_features.append(add_noise(features))\n",
    "        augmented_features.append(scale_features(features))\n",
    "        augmented_features.append(pitch_shift(features))\n",
    "        augmented_features.append(equalize(features))\n",
    "        augmented_labels.extend(labels)\n",
    "        augmented_labels.extend(labels)\n",
    "        augmented_labels.extend(labels)\n",
    "        augmented_labels.extend(labels)\n",
    "    return np.vstack(augmented_features), np.array(augmented_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to apply data augmentation to audio feature datasets. For each file, we augment the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data saved to augmented_datasets/urbansounds_features_fold1.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold2.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold3.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold4.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold5.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold6.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold7.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold8.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold9.csv\n",
      "Augmented data saved to augmented_datasets/urbansounds_features_fold10.csv\n"
     ]
    }
   ],
   "source": [
    "def augment_csv_files(files, output_dir=\"augmented_datasets\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for file in files:\n",
    "        data = pd.read_csv(file)\n",
    "        labels = data.pop('Label').values\n",
    "        features = data.values\n",
    "        \n",
    "        # Perform data augmentation\n",
    "        aug_features, aug_labels = augment_data(features, labels, augmentation_count=2)\n",
    "        \n",
    "        # Combine original data with augmented data\n",
    "        combined_features = np.vstack((features, aug_features))\n",
    "        combined_labels = np.hstack((labels, aug_labels))\n",
    "        \n",
    "        # Save the new CSV\n",
    "        combined_data = pd.DataFrame(combined_features, columns=data.columns)\n",
    "        combined_data['Label'] = combined_labels\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file))\n",
    "        combined_data.to_csv(output_file, index=False)\n",
    "        print(f\"Augmented data saved to {output_file}\")\n",
    "\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "augment_csv_files(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform **10-fold cross-validation on the augmented datasets**. This involves splitting the data into folds and using a specific schema for training, validation, and testing.\n",
    "\n",
    "---\n",
    "\n",
    "**Cross-Validation Schema**:\n",
    "   - **1 Fold** for **Testing**: Original data only.\n",
    "   - **1 Fold** for **Validation**: Original data only.\n",
    "   - **8 Folds** for **Training**: Augmented data.\n",
    "\n",
    "---\n",
    "\n",
    ". Train the model using the augmented training set and the best configuration (`best_config`).\n",
    "\n",
    ". Evaluate the model on the test set after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Fold: 1, Validation Fold: 2, Test Accuracy: 0.5979729890823364\n",
      "Test Fold: 1, Validation Fold: 3, Test Accuracy: 0.5777027010917664\n",
      "Test Fold: 1, Validation Fold: 4, Test Accuracy: 0.625\n",
      "Test Fold: 1, Validation Fold: 5, Test Accuracy: 0.6340090036392212\n",
      "Test Fold: 1, Validation Fold: 6, Test Accuracy: 0.5664414167404175\n",
      "Test Fold: 1, Validation Fold: 7, Test Accuracy: 0.6531531810760498\n",
      "Test Fold: 1, Validation Fold: 8, Test Accuracy: 0.5990990996360779\n",
      "Test Fold: 1, Validation Fold: 9, Test Accuracy: 0.587837815284729\n",
      "Test Fold: 2, Validation Fold: 1, Test Accuracy: 0.5318918824195862\n",
      "Test Fold: 2, Validation Fold: 3, Test Accuracy: 0.5145946145057678\n",
      "Test Fold: 2, Validation Fold: 4, Test Accuracy: 0.5589188933372498\n",
      "Test Fold: 2, Validation Fold: 5, Test Accuracy: 0.49837836623191833\n",
      "Test Fold: 2, Validation Fold: 6, Test Accuracy: 0.500540554523468\n",
      "Test Fold: 2, Validation Fold: 7, Test Accuracy: 0.5275675654411316\n",
      "Test Fold: 2, Validation Fold: 8, Test Accuracy: 0.5091891884803772\n",
      "Test Fold: 2, Validation Fold: 9, Test Accuracy: 0.4745945930480957\n",
      "Test Fold: 3, Validation Fold: 1, Test Accuracy: 0.4707070589065552\n",
      "Test Fold: 3, Validation Fold: 2, Test Accuracy: 0.5414141416549683\n",
      "Test Fold: 3, Validation Fold: 4, Test Accuracy: 0.5292929410934448\n",
      "Test Fold: 3, Validation Fold: 5, Test Accuracy: 0.5626262426376343\n",
      "Test Fold: 3, Validation Fold: 6, Test Accuracy: 0.5797979831695557\n",
      "Test Fold: 3, Validation Fold: 7, Test Accuracy: 0.581818163394928\n",
      "Test Fold: 3, Validation Fold: 8, Test Accuracy: 0.5787878632545471\n",
      "Test Fold: 3, Validation Fold: 9, Test Accuracy: 0.5767676830291748\n",
      "Test Fold: 4, Validation Fold: 1, Test Accuracy: 0.6485042572021484\n",
      "Test Fold: 4, Validation Fold: 2, Test Accuracy: 0.6613247990608215\n",
      "Test Fold: 4, Validation Fold: 3, Test Accuracy: 0.6591880321502686\n",
      "Test Fold: 4, Validation Fold: 5, Test Accuracy: 0.6634615659713745\n",
      "Test Fold: 4, Validation Fold: 6, Test Accuracy: 0.6741452813148499\n",
      "Test Fold: 4, Validation Fold: 7, Test Accuracy: 0.6933760643005371\n",
      "Test Fold: 4, Validation Fold: 8, Test Accuracy: 0.6335470080375671\n",
      "Test Fold: 4, Validation Fold: 9, Test Accuracy: 0.6784188151359558\n",
      "Test Fold: 5, Validation Fold: 1, Test Accuracy: 0.5091130137443542\n",
      "Test Fold: 5, Validation Fold: 2, Test Accuracy: 0.5249088406562805\n",
      "Test Fold: 5, Validation Fold: 3, Test Accuracy: 0.49939247965812683\n",
      "Test Fold: 5, Validation Fold: 4, Test Accuracy: 0.48724180459976196\n",
      "Test Fold: 5, Validation Fold: 6, Test Accuracy: 0.512758195400238\n",
      "Test Fold: 5, Validation Fold: 7, Test Accuracy: 0.5103280544281006\n",
      "Test Fold: 5, Validation Fold: 8, Test Accuracy: 0.49939247965812683\n",
      "Test Fold: 5, Validation Fold: 9, Test Accuracy: 0.5261239409446716\n",
      "Test Fold: 6, Validation Fold: 1, Test Accuracy: 0.5011933445930481\n",
      "Test Fold: 6, Validation Fold: 2, Test Accuracy: 0.5453460812568665\n",
      "Test Fold: 6, Validation Fold: 3, Test Accuracy: 0.5584725737571716\n",
      "Test Fold: 6, Validation Fold: 4, Test Accuracy: 0.5704057216644287\n",
      "Test Fold: 6, Validation Fold: 5, Test Accuracy: 0.5715990662574768\n",
      "Test Fold: 6, Validation Fold: 7, Test Accuracy: 0.5751789808273315\n",
      "Test Fold: 6, Validation Fold: 8, Test Accuracy: 0.5751789808273315\n",
      "Test Fold: 6, Validation Fold: 9, Test Accuracy: 0.5059666037559509\n",
      "Test Fold: 7, Validation Fold: 1, Test Accuracy: 0.6129032373428345\n",
      "Test Fold: 7, Validation Fold: 2, Test Accuracy: 0.6451612710952759\n",
      "Test Fold: 7, Validation Fold: 3, Test Accuracy: 0.6476426720619202\n",
      "Test Fold: 7, Validation Fold: 4, Test Accuracy: 0.578163743019104\n",
      "Test Fold: 7, Validation Fold: 5, Test Accuracy: 0.6377171277999878\n",
      "Test Fold: 7, Validation Fold: 6, Test Accuracy: 0.6042183637619019\n",
      "Test Fold: 7, Validation Fold: 8, Test Accuracy: 0.6265508532524109\n",
      "Test Fold: 7, Validation Fold: 9, Test Accuracy: 0.6426799297332764\n",
      "Test Fold: 8, Validation Fold: 1, Test Accuracy: 0.6397058963775635\n",
      "Test Fold: 8, Validation Fold: 2, Test Accuracy: 0.6691176295280457\n",
      "Test Fold: 8, Validation Fold: 3, Test Accuracy: 0.6764705777168274\n",
      "Test Fold: 8, Validation Fold: 4, Test Accuracy: 0.6458333134651184\n",
      "Test Fold: 8, Validation Fold: 5, Test Accuracy: 0.6899510025978088\n",
      "Test Fold: 8, Validation Fold: 6, Test Accuracy: 0.6973039507865906\n",
      "Test Fold: 8, Validation Fold: 7, Test Accuracy: 0.6678921580314636\n",
      "Test Fold: 8, Validation Fold: 9, Test Accuracy: 0.6446078419685364\n",
      "Test Fold: 9, Validation Fold: 1, Test Accuracy: 0.6105137467384338\n",
      "Test Fold: 9, Validation Fold: 2, Test Accuracy: 0.581839919090271\n",
      "Test Fold: 9, Validation Fold: 3, Test Accuracy: 0.6248506307601929\n",
      "Test Fold: 9, Validation Fold: 4, Test Accuracy: 0.6356033682823181\n",
      "Test Fold: 9, Validation Fold: 5, Test Accuracy: 0.6678614020347595\n",
      "Test Fold: 9, Validation Fold: 6, Test Accuracy: 0.6415770649909973\n",
      "Test Fold: 9, Validation Fold: 7, Test Accuracy: 0.6415770649909973\n",
      "Test Fold: 9, Validation Fold: 8, Test Accuracy: 0.6236559152603149\n",
      "Mean Test Accuracy: 0.5905009528828992, Std Dev: 0.06223620813701775\n",
      "Final Mean Accuracy: 0.5905009528828992, Final Std Dev: 0.06223620813701775\n"
     ]
    }
   ],
   "source": [
    "def load_augmented_data(fold_number, augmented_files):\n",
    "    return load_fold_data(fold_number, augmented_files)\n",
    "\n",
    "def cross_validate_with_test(files, augmented_files, best_config):\n",
    "    folds = list(range(1,len(files)))\n",
    "    combinations = [(test, val) for test, val in itertools.permutations(folds, 2)]\n",
    "    \n",
    "    test_accuracies = []\n",
    "\n",
    "    for test_fold, val_fold in combinations:\n",
    "        X_test, y_test = load_fold_data(test_fold, files)\n",
    "        X_val, y_val = load_fold_data(val_fold, files)\n",
    "\n",
    "        train_folds = [i for i in folds if i != test_fold and i != val_fold]\n",
    "        X_train, y_train = [], []\n",
    "        for fold in train_folds:\n",
    "            X_temp, y_temp = load_augmented_data(fold, augmented_files)\n",
    "            X_train.append(X_temp)\n",
    "            y_train.append(y_temp)\n",
    "        \n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "        accuracy = train_evaluate_model(best_config, X_train, y_train, X_val, y_val)\n",
    "\n",
    "        test_loss, test_accuracy = evaluate_on_test(best_config, X_train, y_train, X_test, y_test)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f\"Test Fold: {test_fold}, Validation Fold: {val_fold}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    mean_accuracy = np.mean(test_accuracies)\n",
    "    std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "    print(f\"Mean Test Accuracy: {mean_accuracy}, Std Dev: {std_accuracy}\")\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "def evaluate_on_test(config, X_train, y_train, X_test, y_test):\n",
    "    model = MLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10,\n",
    "        hidden_units=config['hidden_units'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        activations=config['activations'],\n",
    "        regularization_type=config.get('regularization_type', None),\n",
    "        regularization_value=config.get('regularization_value', 0.01)\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(X_train, y_train, batch_size=config['batch_size'], epochs=config['epochs'], verbose=0)\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "    augmented_files = [f'augmented_datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "    \n",
    "    mean_accuracy, std_accuracy = cross_validate_with_test(files, augmented_files, best_config)\n",
    "    print(f\"Final Mean Accuracy: {mean_accuracy}, Final Std Dev: {std_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Fold: 1, Validation Fold: 2, Test Accuracy: 0.5641891956329346\n",
      "Test Fold: 1, Validation Fold: 3, Test Accuracy: 0.5168918967247009\n",
      "Test Fold: 1, Validation Fold: 4, Test Accuracy: 0.5563063025474548\n",
      "Test Fold: 1, Validation Fold: 5, Test Accuracy: 0.559684693813324\n",
      "Test Fold: 1, Validation Fold: 6, Test Accuracy: 0.5551801919937134\n",
      "Test Fold: 1, Validation Fold: 7, Test Accuracy: 0.5867117047309875\n",
      "Test Fold: 1, Validation Fold: 8, Test Accuracy: 0.5529279112815857\n",
      "Test Fold: 1, Validation Fold: 9, Test Accuracy: 0.5394144058227539\n",
      "Test Fold: 2, Validation Fold: 1, Test Accuracy: 0.5405405163764954\n",
      "Test Fold: 2, Validation Fold: 3, Test Accuracy: 0.5372973084449768\n",
      "Test Fold: 2, Validation Fold: 4, Test Accuracy: 0.591351330280304\n",
      "Test Fold: 2, Validation Fold: 5, Test Accuracy: 0.5956756472587585\n",
      "Test Fold: 2, Validation Fold: 6, Test Accuracy: 0.6064864993095398\n",
      "Test Fold: 2, Validation Fold: 7, Test Accuracy: 0.5816216468811035\n",
      "Test Fold: 2, Validation Fold: 8, Test Accuracy: 0.6010810732841492\n",
      "Test Fold: 2, Validation Fold: 9, Test Accuracy: 0.5794594883918762\n",
      "Test Fold: 3, Validation Fold: 1, Test Accuracy: 0.5717171430587769\n",
      "Test Fold: 3, Validation Fold: 2, Test Accuracy: 0.5878787636756897\n",
      "Test Fold: 3, Validation Fold: 4, Test Accuracy: 0.5808081030845642\n",
      "Test Fold: 3, Validation Fold: 5, Test Accuracy: 0.5444444417953491\n",
      "Test Fold: 3, Validation Fold: 6, Test Accuracy: 0.6181818246841431\n",
      "Test Fold: 3, Validation Fold: 7, Test Accuracy: 0.581818163394928\n",
      "Test Fold: 3, Validation Fold: 8, Test Accuracy: 0.5666666626930237\n",
      "Test Fold: 3, Validation Fold: 9, Test Accuracy: 0.6010100841522217\n",
      "Test Fold: 4, Validation Fold: 1, Test Accuracy: 0.6559829115867615\n",
      "Test Fold: 4, Validation Fold: 2, Test Accuracy: 0.6217948794364929\n",
      "Test Fold: 4, Validation Fold: 3, Test Accuracy: 0.5972222089767456\n",
      "Test Fold: 4, Validation Fold: 5, Test Accuracy: 0.622863233089447\n",
      "Test Fold: 4, Validation Fold: 6, Test Accuracy: 0.6153846383094788\n",
      "Test Fold: 4, Validation Fold: 7, Test Accuracy: 0.6153846383094788\n",
      "Test Fold: 4, Validation Fold: 8, Test Accuracy: 0.6014957427978516\n",
      "Test Fold: 4, Validation Fold: 9, Test Accuracy: 0.6121794581413269\n",
      "Test Fold: 5, Validation Fold: 1, Test Accuracy: 0.5394896864891052\n",
      "Test Fold: 5, Validation Fold: 2, Test Accuracy: 0.5297691226005554\n",
      "Test Fold: 5, Validation Fold: 3, Test Accuracy: 0.5467800498008728\n",
      "Test Fold: 5, Validation Fold: 4, Test Accuracy: 0.4969623386859894\n",
      "Test Fold: 5, Validation Fold: 6, Test Accuracy: 0.5249088406562805\n",
      "Test Fold: 5, Validation Fold: 7, Test Accuracy: 0.5297691226005554\n",
      "Test Fold: 5, Validation Fold: 8, Test Accuracy: 0.5188335180282593\n",
      "Test Fold: 5, Validation Fold: 9, Test Accuracy: 0.543134868144989\n",
      "Test Fold: 6, Validation Fold: 1, Test Accuracy: 0.5489259958267212\n",
      "Test Fold: 6, Validation Fold: 2, Test Accuracy: 0.5322195887565613\n",
      "Test Fold: 6, Validation Fold: 3, Test Accuracy: 0.5453460812568665\n",
      "Test Fold: 6, Validation Fold: 4, Test Accuracy: 0.5513126254081726\n",
      "Test Fold: 6, Validation Fold: 5, Test Accuracy: 0.535799503326416\n",
      "Test Fold: 6, Validation Fold: 7, Test Accuracy: 0.5525059700012207\n",
      "Test Fold: 6, Validation Fold: 8, Test Accuracy: 0.5692124366760254\n",
      "Test Fold: 6, Validation Fold: 9, Test Accuracy: 0.5202863812446594\n",
      "Test Fold: 7, Validation Fold: 1, Test Accuracy: 0.6141439080238342\n",
      "Test Fold: 7, Validation Fold: 2, Test Accuracy: 0.6178659796714783\n",
      "Test Fold: 7, Validation Fold: 3, Test Accuracy: 0.6203473806381226\n",
      "Test Fold: 7, Validation Fold: 4, Test Accuracy: 0.6129032373428345\n",
      "Test Fold: 7, Validation Fold: 5, Test Accuracy: 0.6091811656951904\n",
      "Test Fold: 7, Validation Fold: 6, Test Accuracy: 0.6439206004142761\n",
      "Test Fold: 7, Validation Fold: 8, Test Accuracy: 0.6166253089904785\n",
      "Test Fold: 7, Validation Fold: 9, Test Accuracy: 0.6166253089904785\n",
      "Test Fold: 8, Validation Fold: 1, Test Accuracy: 0.5441176295280457\n",
      "Test Fold: 8, Validation Fold: 2, Test Accuracy: 0.5772058963775635\n",
      "Test Fold: 8, Validation Fold: 3, Test Accuracy: 0.5894607901573181\n",
      "Test Fold: 8, Validation Fold: 4, Test Accuracy: 0.5698529481887817\n",
      "Test Fold: 8, Validation Fold: 5, Test Accuracy: 0.5992646813392639\n",
      "Test Fold: 8, Validation Fold: 6, Test Accuracy: 0.5428921580314636\n",
      "Test Fold: 8, Validation Fold: 7, Test Accuracy: 0.5698529481887817\n",
      "Test Fold: 8, Validation Fold: 9, Test Accuracy: 0.5428921580314636\n",
      "Test Fold: 9, Validation Fold: 1, Test Accuracy: 0.6093189716339111\n",
      "Test Fold: 9, Validation Fold: 2, Test Accuracy: 0.6248506307601929\n",
      "Test Fold: 9, Validation Fold: 3, Test Accuracy: 0.5985662937164307\n",
      "Test Fold: 9, Validation Fold: 4, Test Accuracy: 0.5973715782165527\n",
      "Test Fold: 9, Validation Fold: 5, Test Accuracy: 0.6140979528427124\n",
      "Test Fold: 9, Validation Fold: 6, Test Accuracy: 0.6081242561340332\n",
      "Test Fold: 9, Validation Fold: 7, Test Accuracy: 0.6559139490127563\n",
      "Test Fold: 9, Validation Fold: 8, Test Accuracy: 0.5842294096946716\n",
      "Mean Test Accuracy: 0.578118527515067, Std Dev: 0.03601944388853436\n",
      "Final Mean Accuracy: 0.578118527515067, Final Std Dev: 0.03601944388853436\n"
     ]
    }
   ],
   "source": [
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "augmented_files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "\n",
    "mean_accuracy, std_accuracy = cross_validate_with_test(files, augmented_files, best_config)\n",
    "print(f\"Final Mean Accuracy: {mean_accuracy}, Final Std Dev: {std_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
