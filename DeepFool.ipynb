{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a30c9b0",
   "metadata": {},
   "source": [
    "### Deep Fool Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113174d9",
   "metadata": {},
   "source": [
    "We have achieved some results with the CNN and MLP models, but we would like to explore how these models perform when faced with adversarial examples. To do so, we aply DeepFool.\n",
    "\n",
    "**DeepFool** is a widely used method for generating adversarial examples, designed to evaluate the robustness of machine learning models, particularly in classification tasks. \n",
    "\n",
    "By iteratively finding the minimal perturbation needed to alter a model's prediction, DeepFool provides insights into a **model's vulnerability to adversarial attacks**. This method is crucial for developing more **robust and reliable models**, as it helps identify potential weaknesses and informs strategies for improving their defense mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b642f6",
   "metadata": {},
   "source": [
    "![Descrição da imagem](./images/deepfool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfcf90",
   "metadata": {},
   "source": [
    "\n",
    "1. **Initialization:** Starting with the original image \\( $x_0$ \\), and setting the iteration counter \\($ i $\\) to 0.\n",
    "\n",
    "2. **Perturbation Calculation:** For each iteration:\n",
    "   - The algorithm calculates the perturbation required to change the model's prediction, iteratively adjusting the image.\n",
    "   - It computes the gradients and the necessary perturbation \\( $r_i $\\) for the class boundary.\n",
    "\n",
    "3. **Repeat Until Misclassification:** The algorithm repeats this process until the image is misclassified by the model.\n",
    "\n",
    "4. **Return Perturbation:** The final perturbation is the sum of all the adjustments \\( $r_i$ \\) made during the iterations.\n",
    "\n",
    "The result is the minimal perturbation $ r $ that causes a misclassification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d1ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of relevant libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, regularizers, optimizers\n",
    "from tensorflow.python.client import device_lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import librosa\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b56bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(model, x, target_class):\n",
    "    \"\"\"\n",
    "    Calcula o gradiente da saída da classe-alvo com respeito à entrada.\n",
    "\n",
    "    Args:\n",
    "        model: O modelo neural treinado.\n",
    "        x: Entrada para o modelo (tensor).\n",
    "        target_class: Índice da classe-alvo.\n",
    "\n",
    "    Returns:\n",
    "        Gradiente calculado.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        logits = model(x)\n",
    "        target_logits = logits[:, target_class]\n",
    "    return tape.gradient(target_logits, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9076828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def deepfool(model, x0, eta=1e-2, max_iter=50, num_classes=10):\n",
    "    \"\"\"\n",
    "    Implementa o algoritmo DeepFool para calcular a menor perturbação.\n",
    "\n",
    "    Args:\n",
    "        model: O modelo neural treinado.\n",
    "        x0: Entrada inicial (tensor).\n",
    "        eta: Parâmetro de overshoot.\n",
    "        max_iter: Número máximo de iterações.\n",
    "        num_classes: Número de classes no modelo.\n",
    "\n",
    "    Returns:\n",
    "        r_sum: Perturbação acumulada.\n",
    "        loop_i: Número de iterações realizadas.\n",
    "        label_xi: Nova previsão após a perturbação.\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x0, dtype=tf.float32)\n",
    "    r_sum = tf.zeros_like(x)\n",
    "    label_xi = tf.argmax(model(x), axis=1).numpy()[0]\n",
    "\n",
    "    for loop_i in range(max_iter):\n",
    "        gradients = []\n",
    "        logits = model(x)\n",
    "        current_label = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "        if current_label != label_xi:\n",
    "            break\n",
    "\n",
    "        for k in range(num_classes):\n",
    "            grad = get_gradient(model, x, k)\n",
    "            gradients.append(grad)\n",
    "\n",
    "        gradients = tf.stack(gradients)\n",
    "        logits = tf.squeeze(logits)\n",
    "\n",
    "        smallest_perturbation = float('inf')\n",
    "        for k in range(num_classes):\n",
    "            if k == label_xi:\n",
    "                continue\n",
    "            w_k = gradients[k] - gradients[label_xi]\n",
    "            f_k = logits[k] - logits[label_xi]\n",
    "            perturbation = tf.abs(f_k) / tf.norm(w_k, ord=2)\n",
    "            if perturbation < smallest_perturbation:\n",
    "                smallest_perturbation = perturbation\n",
    "                r_i = (perturbation + eta) * w_k / tf.norm(w_k, ord=2)\n",
    "\n",
    "        r_sum += r_i\n",
    "        x = x + r_i\n",
    "\n",
    "    return r_sum, loop_i, current_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72d9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_robustness(r, x):\n",
    "    \"\"\"\n",
    "    Calcula a robustez de um exemplo.\n",
    "\n",
    "    Args:\n",
    "        r: Perturbação adversarial aplicada.\n",
    "        x: Entrada original.\n",
    "\n",
    "    Returns:\n",
    "        Valor de robustez (ρ).\n",
    "    \"\"\"\n",
    "    norm_r = tf.norm(r)\n",
    "    norm_x = tf.norm(x)\n",
    "    return norm_r / norm_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8803013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_robustness(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Avalia a robustez média do modelo em relação ao conjunto de testes.\n",
    "\n",
    "    Args:\n",
    "        model: O modelo neural treinado.\n",
    "        X_test: Conjunto de dados de teste.\n",
    "        y_test: Labels do conjunto de teste.\n",
    "\n",
    "    Returns:\n",
    "        Média e desvio padrão da robustez.\n",
    "    \"\"\"\n",
    "    rho_values = []\n",
    "    for i in range(len(X_test)):\n",
    "        x = tf.expand_dims(X_test[i], axis=0)\n",
    "        r, _, _ = deepfool(model, x)\n",
    "        rho = example_robustness(r, x)\n",
    "        rho_values.append(rho.numpy())\n",
    "    \n",
    "    mean_rho = np.mean(rho_values)\n",
    "    std_rho = np.std(rho_values)\n",
    "    return mean_rho, std_rho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a00b8f-86f5-4ab3-8ab6-a6bbf6b70681",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Avaliar robustez do modelo\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean_rho, std_rho \u001b[38;5;241m=\u001b[39m model_robustness(\u001b[43mmodel\u001b[49m, X_test, y_test)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMedium robustness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_rho\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard deviation of robustness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_rho\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Avaliar robustez do modelo\n",
    "mean_rho, std_rho = model_robustness(model, X_test, y_test)\n",
    "print(f\"Medium robustness: {mean_rho:.4f}\")\n",
    "print(f\"Standard deviation of robustness: {std_rho:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ea02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1875 - loss: 2.2475 - val_accuracy: 0.2917 - val_loss: 2.1037\n",
      "Epoch 2/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3056 - loss: 2.0502 - val_accuracy: 0.3567 - val_loss: 1.9065\n",
      "Epoch 3/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4009 - loss: 1.8247 - val_accuracy: 0.4420 - val_loss: 1.7311\n",
      "Epoch 4/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4830 - loss: 1.6741 - val_accuracy: 0.4420 - val_loss: 1.6252\n",
      "Epoch 5/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5298 - loss: 1.5154 - val_accuracy: 0.4943 - val_loss: 1.4976\n",
      "Epoch 6/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5504 - loss: 1.4208 - val_accuracy: 0.5121 - val_loss: 1.4090\n",
      "Epoch 7/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5720 - loss: 1.3396 - val_accuracy: 0.5580 - val_loss: 1.3154\n",
      "Epoch 8/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5914 - loss: 1.2700 - val_accuracy: 0.6038 - val_loss: 1.2624\n",
      "Epoch 9/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6153 - loss: 1.1980 - val_accuracy: 0.5694 - val_loss: 1.2334\n",
      "Epoch 10/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6217 - loss: 1.1582 - val_accuracy: 0.6268 - val_loss: 1.1823\n",
      "Epoch 11/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6329 - loss: 1.1042 - val_accuracy: 0.6522 - val_loss: 1.1494\n",
      "Epoch 12/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6415 - loss: 1.0856 - val_accuracy: 0.6420 - val_loss: 1.1320\n",
      "Epoch 13/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6376 - loss: 1.0864 - val_accuracy: 0.6815 - val_loss: 1.0905\n",
      "Epoch 14/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6535 - loss: 1.0493 - val_accuracy: 0.7019 - val_loss: 1.0757\n",
      "Epoch 15/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6574 - loss: 1.0345 - val_accuracy: 0.6879 - val_loss: 1.0265\n",
      "Epoch 16/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6635 - loss: 1.0138 - val_accuracy: 0.7057 - val_loss: 1.0247\n",
      "Epoch 17/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6632 - loss: 1.0095 - val_accuracy: 0.7261 - val_loss: 0.9899\n",
      "Epoch 18/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6784 - loss: 0.9641 - val_accuracy: 0.6943 - val_loss: 0.9874\n",
      "Epoch 19/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6800 - loss: 0.9541 - val_accuracy: 0.7261 - val_loss: 0.9442\n",
      "Epoch 20/20\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6953 - loss: 0.9209 - val_accuracy: 0.7274 - val_loss: 0.9322\n"
     ]
    }
   ],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units, dropout_rate, activations, regularization_type=None, regularization_value=0.01):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = []\n",
    "        self.regularization_type = regularization_type\n",
    "        self.regularization_value = regularization_value\n",
    "\n",
    "        for units, activation in zip(hidden_units, activations):\n",
    "            self.hidden_layers.append(\n",
    "                tf.keras.layers.Dense(units, activation=activation)\n",
    "            )\n",
    "            self.hidden_layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, activation='softmax')  \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def compute_regularization_loss(self):\n",
    "        regularization_loss = 0.0\n",
    "        if self.regularization_type:\n",
    "            for layer in self.hidden_layers:\n",
    "                if isinstance(layer, tf.keras.layers.Dense):\n",
    "                    weights = layer.kernel\n",
    "                    if self.regularization_type == 'l1':\n",
    "                        regularization_loss += tf.reduce_sum(tf.abs(weights)) * self.regularization_value\n",
    "                    elif self.regularization_type == 'l2':\n",
    "                        regularization_loss += tf.reduce_sum(tf.square(weights)) * self.regularization_value\n",
    "        return regularization_loss\n",
    "\n",
    "def load_fold_data(fold_index, files):\n",
    "    # Adjust fold_index to be zero-based\n",
    "    data = pd.read_csv(files[fold_index-1]).to_numpy()\n",
    "\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Warning: Missing values detected in file {files[fold_index - 1]}.\")\n",
    "        data = data[~np.isnan(data).any(axis=1)]  # Remove rows with NaN values\n",
    "    X = data[:, :-1]  # Features\n",
    "    y = data[:, -1].astype(int)  # Labels\n",
    "    if (y < 0).any() or (y >= 10).any():\n",
    "        raise ValueError(f\"Invalid label values detected in file {files[fold_index - 1]}. Labels: {np.unique(y)}\")\n",
    "    return X, y\n",
    "\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "\n",
    "# Define the test fold\n",
    "fold_test = 1\n",
    "X_test, y_test = load_fold_data(fold_test, files)\n",
    "\n",
    "# Define the training folds\n",
    "X_train, y_train = [], []\n",
    "for i in range(1, 11):  # Total of 10 folds\n",
    "    if i != fold_test:\n",
    "        X_temp, y_temp = load_fold_data(i, files)\n",
    "        X_train.append(X_temp)\n",
    "        y_train.append(y_temp)\n",
    "\n",
    "# Concatenate the training data\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "# Hyperparameters\n",
    "best_config = {\n",
    "    'hidden_units': [256, 128, 64],\n",
    "    'activations': ['relu', 'relu', 'relu'],\n",
    "    'dropout_rate': 0.3,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.0001,\n",
    "    'regularization_type': None,\n",
    "    'regularization_value': 0.01\n",
    "}\n",
    "\n",
    "# Initialize and train the model\n",
    "model = MLP(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=10,  # Classes from 0 to 9\n",
    "    hidden_units=best_config['hidden_units'],\n",
    "    dropout_rate=best_config['dropout_rate'],\n",
    "    activations=best_config['activations']\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_config['learning_rate']),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Example validation split\n",
    "X_val, y_val = X_train[:len(X_train)//10], y_train[:len(y_train)//10]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=best_config['batch_size'],\n",
    "    epochs=best_config['epochs']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "067d6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fold_data(fold_number, files):\n",
    "    data = pd.read_csv(files[fold_number - 1]) \n",
    "    if data.empty:\n",
    "        print(f\"Erro: O arquivo {files[fold_number - 1]} está vazio ou não foi carregado corretamente.\")\n",
    "    labels = data.pop('Label').values\n",
    "    features = data.values\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def train_evaluate_model(config, X_train, y_train, X_val, y_val):\n",
    "    model = MLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10,\n",
    "        hidden_units=config['hidden_units'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        activations=config['activations'],\n",
    "        regularization_type=config.get('regularization_type', None),\n",
    "        regularization_value=config.get('regularization_value', 0.01)\n",
    "    )\n",
    "    \n",
    "    def loss_with_regularization(y_true, y_pred):\n",
    "        base_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        regularization_loss = model.compute_regularization_loss()\n",
    "        return base_loss + regularization_loss\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss=loss_with_regularization,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    return history \n",
    "\n",
    "def deepfool_mlp(model, x0, y0, max_iter=50, epsilon=1e-6):\n",
    "    x_adv = tf.identity(x0)  # Tensor inicial\n",
    "    for i in range(max_iter):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            logits = model(x_adv)\n",
    "            pred_label = tf.argmax(logits, axis=-1).numpy()[0]\n",
    "\n",
    "        if pred_label != y0:\n",
    "            return x_adv.numpy()  # Retorna o adversarial se o modelo errar\n",
    "\n",
    "        gradients = tape.gradient(logits, x_adv).numpy()[0]\n",
    "        logits = logits.numpy()[0]\n",
    "        current_label = pred_label\n",
    "\n",
    "        perturbations = []\n",
    "        for k in range(len(logits)):\n",
    "            if k != y0:\n",
    "                diff_grad = gradients[k] - gradients[y0]\n",
    "                diff_logits = logits[k] - logits[y0]\n",
    "                perturbation = abs(diff_logits) / (np.linalg.norm(diff_grad) + epsilon)\n",
    "                perturbations.append((perturbation, k))\n",
    "\n",
    "        perturbations.sort(key=lambda x: x[0])  # Ordena pelo menor deslocamento necessário\n",
    "        r_min, _ = perturbations[0]\n",
    "        x_adv += r_min * gradients\n",
    "\n",
    "    return x_adv.numpy()  # Retorna a última iteração como fallback\n",
    "\n",
    "\n",
    "def cross_validation_mlp_deepfool(datasets, model_builder, params):\n",
    "    folds = list(range(1, 11))\n",
    "    accuracy_values = []\n",
    "    loss_values = []\n",
    "    robustness_values = []\n",
    "\n",
    "    for test_fold in folds:\n",
    "        train_val_folds = [fold for fold in folds if fold != test_fold]\n",
    "        train_folds = train_val_folds[:-1]\n",
    "        val_fold = train_val_folds[-1]\n",
    "        \n",
    "        # Carregar dados\n",
    "        X_train, y_train = [], []\n",
    "        for fold in train_folds:\n",
    "            X_temp, y_temp = load_fold_data(fold, files)\n",
    "            X_train.append(X_temp)\n",
    "            y_train.append(y_temp)\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "        X_val, y_val = load_fold_data(val_fold, files)\n",
    "        X_test, y_test = load_fold_data(test_fold, files)\n",
    "\n",
    "        # Treinar modelo\n",
    "        model = model_builder(input_dim=X_train.shape[1], best_config=best_config)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['epochs'],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Avaliação de desempenho\n",
    "        fold_loss, fold_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        accuracy_values.append(fold_accuracy)\n",
    "        loss_values.append(fold_loss)\n",
    "\n",
    "        # Robustez com DeepFool\n",
    "        adversarial_success = 0\n",
    "        for idx in range(len(X_test)):\n",
    "            x0 = np.expand_dims(X_test[idx], axis=0)\n",
    "            y0 = y_test[idx]\n",
    "            x_adv = deepfool_mlp(model, x0, y0)\n",
    "            adv_pred = tf.argmax(model(x_adv), axis=-1).numpy()[0]\n",
    "\n",
    "            if adv_pred != y0:\n",
    "                adversarial_success += 1\n",
    "\n",
    "        robustness = 1 - (adversarial_success / len(X_test))\n",
    "        robustness_values.append(robustness)\n",
    "        print(f\"Fold {test_fold}: Accuracy={fold_accuracy:.4f}, Loss={fold_loss:.4f}, Robustness={robustness:.4f}\")\n",
    "\n",
    "    return accuracy_values, loss_values, robustness_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "534a0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_units': [256, 128, 64],\n",
    "    'activations': ['relu', 'relu', 'relu'],\n",
    "    'dropout_rate': 0.3,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 0.0001,\n",
    "    'regularization_type': None,\n",
    "    'regularization_value': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "253acf75-5348-40b7-8e8e-a6fd1ec4d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(input_dim, best_config):\n",
    "    \"\"\"\n",
    "    Build and compile an MLP model based on the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        best_config (dict): Dictionary containing hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled MLP model.\n",
    "    \"\"\"\n",
    "    model = MLP(\n",
    "                input_dim=X_train.shape[1],\n",
    "                output_dim=10,\n",
    "                hidden_units=best_config['hidden_units'],\n",
    "                dropout_rate=best_config['dropout_rate'],\n",
    "                activations=best_config['activations'],\n",
    "                regularization_type=best_config.get('regularization_type', None),\n",
    "                regularization_value=best_config.get('regularization_value', 0.01)\n",
    "            )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=best_config['learning_rate']),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "412a6313-52b8-4aa1-a076-044b13305b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all folds into a list\n",
    "files = [f'datasets/urbansounds_features_fold{i}.csv' for i in range(1, 11)]\n",
    "folds = [pd.read_csv(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a6fb1ec-19bf-4d18-9293-726c47948431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy=0.6472, Loss=1.1020, Robustness=0.5613\n",
      "Fold 2: Accuracy=0.5405, Loss=1.3149, Robustness=0.4944\n",
      "Fold 3: Accuracy=0.6000, Loss=1.1624, Robustness=0.5319\n",
      "Fold 4: Accuracy=0.5970, Loss=1.1801, Robustness=0.5131\n",
      "Fold 5: Accuracy=0.6132, Loss=1.0799, Robustness=0.5192\n",
      "Fold 6: Accuracy=0.5699, Loss=1.4149, Robustness=0.5140\n",
      "Fold 7: Accuracy=0.5656, Loss=1.3666, Robustness=0.5179\n",
      "Fold 8: Accuracy=0.6414, Loss=1.2023, Robustness=0.5434\n",
      "Fold 9: Accuracy=0.5172, Loss=1.3547, Robustness=0.4301\n",
      "Fold 10: Accuracy=0.5974, Loss=1.2022, Robustness=0.5030\n",
      "Resultados da Validação Cruzada:\n",
      "Fold 1:\n",
      " - Acurácia: 0.6472\n",
      " - Perda: 1.1020\n",
      " - Robustez: 0.5613\n",
      "Fold 2:\n",
      " - Acurácia: 0.5405\n",
      " - Perda: 1.3149\n",
      " - Robustez: 0.4944\n",
      "Fold 3:\n",
      " - Acurácia: 0.6000\n",
      " - Perda: 1.1624\n",
      " - Robustez: 0.5319\n",
      "Fold 4:\n",
      " - Acurácia: 0.5970\n",
      " - Perda: 1.1801\n",
      " - Robustez: 0.5131\n",
      "Fold 5:\n",
      " - Acurácia: 0.6132\n",
      " - Perda: 1.0799\n",
      " - Robustez: 0.5192\n",
      "Fold 6:\n",
      " - Acurácia: 0.5699\n",
      " - Perda: 1.4149\n",
      " - Robustez: 0.5140\n",
      "Fold 7:\n",
      " - Acurácia: 0.5656\n",
      " - Perda: 1.3666\n",
      " - Robustez: 0.5179\n",
      "Fold 8:\n",
      " - Acurácia: 0.6414\n",
      " - Perda: 1.2023\n",
      " - Robustez: 0.5434\n",
      "Fold 9:\n",
      " - Acurácia: 0.5172\n",
      " - Perda: 1.3547\n",
      " - Robustez: 0.4301\n",
      "Fold 10:\n",
      " - Acurácia: 0.5974\n",
      " - Perda: 1.2022\n",
      " - Robustez: 0.5030\n"
     ]
    }
   ],
   "source": [
    "accuracy_values, loss_values, robustness_values = cross_validation_mlp_deepfool(\n",
    "    datasets=files,            \n",
    "    model_builder=build_mlp_model,  \n",
    "    params=best_config        \n",
    ")\n",
    "\n",
    "print(\"Resultados da Validação Cruzada:\")\n",
    "for i in range(len(accuracy_values)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    print(f\" - Acurácia: {accuracy_values[i]:.4f}\")\n",
    "    print(f\" - Perda: {loss_values[i]:.4f}\")\n",
    "    print(f\" - Robustez: {robustness_values[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7787acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy=0.6804, Loss=1.1354, Robustness=0.5945\n",
      "Fold 2: Accuracy=0.5439, Loss=1.2987, Robustness=0.4899\n",
      "Fold 3: Accuracy=0.6032, Loss=1.1992, Robustness=0.5178\n",
      "Fold 4: Accuracy=0.5808, Loss=1.2251, Robustness=0.4980\n",
      "Fold 5: Accuracy=0.6154, Loss=1.0490, Robustness=0.5331\n",
      "Fold 6: Accuracy=0.5176, Loss=1.5814, Robustness=0.4520\n",
      "Fold 7: Accuracy=0.5525, Loss=1.3745, Robustness=0.5060\n",
      "Fold 8: Accuracy=0.6154, Loss=1.2348, Robustness=0.5186\n",
      "Fold 9: Accuracy=0.5417, Loss=1.3398, Robustness=0.4608\n",
      "Fold 10: Accuracy=0.6045, Loss=1.1937, Robustness=0.5293\n",
      "\n",
      "Cross-Validation Summary:\n",
      "Average Accuracy: 0.5855\n",
      "Average Loss: 1.2632\n",
      "Average Robustness: 0.5100\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mlp_deepfool(datasets, model_builder, params):\n",
    "    folds = list(range(1, 11))\n",
    "    accuracy_values = []\n",
    "    loss_values = []\n",
    "    robustness_values = []\n",
    "\n",
    "    for test_fold in folds:\n",
    "        train_val_folds = [fold for fold in folds if fold != test_fold]\n",
    "        train_folds = train_val_folds[:-1]\n",
    "        val_fold = train_val_folds[-1]\n",
    "        \n",
    "        # Load data\n",
    "        X_train, y_train = [], []\n",
    "        for fold in train_folds:\n",
    "            X_temp, y_temp = load_fold_data(fold, files)\n",
    "            X_train.append(X_temp)\n",
    "            y_train.append(y_temp)\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "        X_val, y_val = load_fold_data(val_fold, files)\n",
    "        X_test, y_test = load_fold_data(test_fold, files)\n",
    "\n",
    "        # Train model\n",
    "        model = model_builder(input_dim=X_train.shape[1], best_config=best_config)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['epochs'],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Evaluate performance\n",
    "        fold_loss, fold_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        accuracy_values.append(fold_accuracy)\n",
    "        loss_values.append(fold_loss)\n",
    "\n",
    "        # Robustness with DeepFool\n",
    "        adversarial_success = 0\n",
    "        for idx in range(len(X_test)):\n",
    "            x0 = np.expand_dims(X_test[idx], axis=0)\n",
    "            y0 = y_test[idx]\n",
    "            x_adv = deepfool_mlp(model, x0, y0)\n",
    "            adv_pred = tf.argmax(model(x_adv), axis=-1).numpy()[0]\n",
    "\n",
    "            if adv_pred != y0:\n",
    "                adversarial_success += 1\n",
    "\n",
    "        robustness = 1 - (adversarial_success / len(X_test))\n",
    "        robustness_values.append(robustness)\n",
    "        print(f\"Fold {test_fold}: Accuracy={fold_accuracy:.4f}, Loss={fold_loss:.4f}, Robustness={robustness:.4f}\")\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_accuracy = np.mean(accuracy_values)\n",
    "    avg_loss = np.mean(loss_values)\n",
    "    avg_robustness = np.mean(robustness_values)\n",
    "\n",
    "    # Display averages\n",
    "    print(\"\\nCross-Validation Summary:\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average Robustness: {avg_robustness:.4f}\")\n",
    "\n",
    "    return accuracy_values, loss_values, robustness_values\n",
    "\n",
    "accuracy_values, loss_values, robustness_values = cross_validation_mlp_deepfool(\n",
    "    datasets=files,            \n",
    "    model_builder=build_mlp_model,  \n",
    "    params=best_config        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ecb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69ba7109-00f5-40e1-b8ff-5015bf13c296",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
